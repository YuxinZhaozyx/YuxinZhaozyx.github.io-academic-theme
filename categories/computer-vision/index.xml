<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>computer vision on YuxinZhao</title>
    <link>https://YuxinZhaozyx.github.io/categories/computer-vision/</link>
    <description>Recent content in computer vision on YuxinZhao</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year} YuxinZhao</copyright>
    <lastBuildDate>Wed, 17 Jul 2019 14:12:08 +0800</lastBuildDate>
    
	    <atom:link href="https://YuxinZhaozyx.github.io/categories/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>NMS / soft-NMS / softer-NMS</title>
      <link>https://YuxinZhaozyx.github.io/post/nms/</link>
      <pubDate>Wed, 17 Jul 2019 14:12:08 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/nms/</guid>
      <description>

&lt;p&gt;目标检测算法包含了三个要素：Backbone + Head + Postprocess，对于Postprocess部分，最早用的是NMS，后面出现了Soft NMS和Softer NMS，本文将分别解释它们的动机和原理。&lt;/p&gt;

&lt;h2 id=&#34;nms&#34;&gt;NMS&lt;/h2&gt;

&lt;p&gt;NMS，它的全称为“non-maximum supression”，中文名“非极大值抑制”。为什么要使用NMS呢？因为在目标检测任务中，不管是one-stage还是two-stage的算法，最终算法都会预测出多个proposals。在后处理部分中，需要对这些proposals做筛选。&lt;/p&gt;

&lt;h3 id=&#34;动机&#34;&gt;动机&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;优先选择分类score较高的proposal；&lt;/li&gt;
&lt;li&gt;跟分类score重叠较多的proposals，可以视为冗余的预测框；&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;步骤&#34;&gt;步骤&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;将算法预测出的所有proposals，按照不同的类别标签分组；&lt;/li&gt;
&lt;li&gt;对于每一个类别的所有proposals，记作$B$，筛选后的proposals集合记作$D$，执行如下操作，

&lt;ul&gt;
&lt;li&gt;a. 选择score最高的proposal，记作$M$，加入到$M$中；&lt;/li&gt;
&lt;li&gt;b. 计算剩余的proposals与$M$之间的&lt;a href=&#34;https://YuxinZhaozyx.github.io/post/what-is-iou&#34;&gt;IoU&lt;/a&gt;，若大于阈值$N_t$ ，则舍弃，否则保留；&lt;/li&gt;
&lt;li&gt;c. 若步骤b中得到的所有proposals为空，则跳回步骤b，否则执行步骤a。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;经过后处理之后，所有类别保留的有效proposals集合为 $S={D_1, D_2, …, D_c}$ ，其中$c$表示目标类别的数量；&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&#34;伪代码&#34;&gt;伪代码&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;image/nms.png&#34; alt=&#34;nms&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;soft-nms&#34;&gt;Soft-NMS&lt;/h2&gt;

&lt;h3 id=&#34;动机-1&#34;&gt;动机&lt;/h3&gt;

&lt;p&gt;由上可见，NMS算法保留score最高的预测框，并将与当前预测框重叠较多的proposals视作冗余，显然，在实际的检测任务中，这种思路有明显的缺点，比如对于稠密物体检测，当同类的两个目标距离较近时，如果使用原生的NMS，就会导致其中一个目标不能被召回，为了提高这种情况下目标检测的召回率，Soft-NMS应运而生。对于Faster-RCNN在MS-COCO数据集上的结果，将NMS改成Soft-NMS，mAP提升了1.1%。&lt;/p&gt;

&lt;h3 id=&#34;算法思想&#34;&gt;算法思想&lt;/h3&gt;

&lt;p&gt;Soft-NMS，原文的标题为&lt;a href=&#34;https://arxiv.org/pdf/1704.04503.pdf&#34; target=&#34;_blank&#34;&gt;“Improving Object Detection With One Line of Code”&lt;/a&gt;。NMS采用“一刀切”的思想，将重叠较多的proposals全部视作冗余，而Soft-NMS，采用了“迂回”战术，它认为重叠较多的proposals也有可能包含有效目标，只不过重叠区域越大可能性越小。参见下图，NMS会将绿色框的score置0，而Soft-NMS会将绿色框的score由0.8下降到0.4，显然Soft-NMS更加合理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/1563347147055.png&#34; alt=&#34;1563347147055&#34; /&gt;&lt;/p&gt;

&lt;p&gt;那么问题来了，怎么建立IoU和score之间的联系呢，文章中给出的公式如下:&lt;/p&gt;

&lt;p&gt;$$
s_i = s_i e^{-\frac{\text{iou}(M, b_i)^2}{\sigma}}, \forall b_i \notin D
$$&lt;/p&gt;

&lt;p&gt;其中$D$表示所有保留的有效框集合, $b_i$表示待过滤的第$i$个预测框，$s_i$为第$i$个预测框对应的分类score。这里使用了高斯函数作为惩罚项，当$iou=0$时，分类score不变，当$0&amp;lt;iou&amp;lt;1$时，分类score会做衰减。以上图为例，绿色框$b_i$和红色框$M$的iou大于0，经过Soft-NMS后该绿色框的分类score由0.8衰减到0.4，可以推断出，如果图中有第2个绿色框，且其与红色框的重叠区域更大时，那么这个新的绿色框的分类score可能由0.8衰减到0.01。&lt;/p&gt;

&lt;h3 id=&#34;步骤-1&#34;&gt;步骤&lt;/h3&gt;

&lt;h4 id=&#34;伪代码-1&#34;&gt;伪代码&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;image/1563347916824.png&#34; alt=&#34;1563347916824&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;softer-nms&#34;&gt;Softer-NMS&lt;/h2&gt;

&lt;h3 id=&#34;动机-2&#34;&gt;动机&lt;/h3&gt;

&lt;h4 id=&#34;现有方法的问题&#34;&gt;现有方法的问题&lt;/h4&gt;

&lt;p&gt;作者使用VGG-16 faster R-CNN测试了MS-COCO数据集中的图片，论文中贴了两张检测失败的代表图片，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/1563348045749.png&#34; alt=&#34;1563348045749&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;左图存在的问题：&lt;/strong&gt;检测出来的2个proposals，沿着y坐标轴方向的定位均不准确；&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;结论：&lt;/strong&gt;检测算法预测出来的proposals的坐标不一定准确；&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;右图存在的问题：&lt;/strong&gt;检测出来的2个proposals，右边的框分类score较高，但是却沿着x坐标轴方向的定位不准确；&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;结论：&lt;/strong&gt;分类score高不一定定位score高，也即classification confidence和 localization confidence不具有一致性。&lt;/p&gt;

&lt;h4 id=&#34;本文解决方法&#34;&gt;本文解决方法&lt;/h4&gt;

&lt;p&gt;针对上面的问题，
1. 既然proposals的坐标不准确，那么即便NMS也无能为力了，所以需要重新设计坐标回归的方式)；
2. 既然分类score高不一定定位score高，那么NMS和Soft-NMS的做法（只基于分类score对proposals做排序）是不准确的，所以需要同时预测出检测框的定位score。&lt;/p&gt;

&lt;h3 id=&#34;算法&#34;&gt;算法&lt;/h3&gt;

&lt;p&gt;Softer-NMS的算法框架如下图，可以看出，它跟fast R-CNN是非常相似的，区别在于回归任务中多了一个Box std分支，这里需要解释一下，比如预测出的bounding box的坐标为$x_1, y_1, x_2, y_2$，该分支会预测出每个坐标的标准差，显然，当坐标的标准差越小时，表明预测得到的坐标值越可信，也即Box std分支用于表征定位任务的置信度。&lt;/p&gt;







&lt;figure&gt;

&lt;img src=&#34;image/1563348994861.png&#34; alt=&#34;fast R-CNN&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    fast R-CNN
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;








&lt;figure&gt;

&lt;img src=&#34;image/1563349684043.png&#34; alt=&#34;faster-NMS&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    faster-NMS
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;h4 id=&#34;定位任务&#34;&gt;定位任务&lt;/h4&gt;

&lt;p&gt;在fast R-CNN中，作者使用的是均方误差函数作为定位损失，总的目的是让定位出的坐标点尽可能逼近groundtruth box。本文中为了在定位坐标同时输出定位score，使用了高斯函数建模坐标点的位置分布，公式如下，&lt;/p&gt;

&lt;p&gt;$$
P_\Theta(x) = \frac1{2\pi \sigma^2} e^{-\frac{(x-x_e)^2}{2\sigma^2}}
$$&lt;/p&gt;

&lt;p&gt;其中，$x_e$为预测的box位置，$\sigma$表示box位置的标准差，衡量了box位置的不确定性。因为groundtruth位置是确定的，所以groundtruth box的坐标为标准差为0的高斯分布，也即Dirac delta函数，公式如下，&lt;/p&gt;

&lt;p&gt;$$
P_D(x) = \delta (x-x_g)
$$&lt;/p&gt;

&lt;p&gt;其中，$x_g$ 为groundtruth box的坐标。&lt;/p&gt;

&lt;h4 id=&#34;定位损失&#34;&gt;定位损失&lt;/h4&gt;

&lt;p&gt;回归任务的目的是让预测框尽可能逼近真实框，也即$P_\theta(x)$和$P_D(x)$为同一分布，衡量概率分布的相似性，自然而然会想到KL散度，关于KL散度的概念，大家可以参见维基百科，值得一提的是，KL散度本身具有不对称性，通常，在实际应用中为了使用对称性，使用的是KL散度的变形形式，但本文中没有这么做。对公式做化简后，最终的简化形式如下，&lt;/p&gt;

&lt;p&gt;$$
L_{reg} = \alpha \left( \left( x_g - x_e \right) - \frac12 \right) - \frac12 \log \left( \alpha + \epsilon \right)
$$&lt;/p&gt;

&lt;h4 id=&#34;后处理&#34;&gt;后处理&lt;/h4&gt;

&lt;p&gt;经过上面的网络部分，Class分支会输出类别score，Box分支会输出box的4个坐标和这4个坐标对应的标准差（定位score），符号表示如下，&lt;/p&gt;

&lt;p&gt;$$
\{ x1_i, y1_i, x2_i, y2_i, s_i, \sigma_{x1, i}, \sigma_{y1, i}, \sigma_{x2, i}, \sigma_{y2, i} \}
$$&lt;/p&gt;

&lt;p&gt;基于这些信息，新的后处理算法如下图，&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/1563351203720.png&#34; alt=&#34;1563351203720&#34; /&gt;&lt;/p&gt;

&lt;p&gt;显然，softer-NMS基于回归出的定位confidence，对所有与$M$的IoU超过阈值$N_t$的proposals，使用加权平均更新其位置坐标，从而达到提高定位精度的目的。因为softer-NMS关注的是单个框的定位精度，而NMS和soft-NMS关注的是单个框的冗余性，显然关注点不同，所以softer-NMS可以和soft-NMS组合使用，此时效果更佳。&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;NMS：&lt;/strong&gt;只适用于图片中目标比较稀疏的场景，即目标之间的间距较大；&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;soft-NMS：&lt;/strong&gt;可以部分解决出现稠密目标的情况；&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;softer-NMS：&lt;/strong&gt;该后处理方法采用&amp;rdquo;bagging&amp;rdquo;的思想，通过后处理提高定位精度，可以和soft-NMS组合使用。&lt;/p&gt;

&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;

&lt;p&gt;[1] &lt;a href=&#34;https://blog.csdn.net/diligent_321/article/details/85859462&#34; target=&#34;_blank&#34;&gt;目标检测后处理：从nms到softer nms&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&#34;https://arxiv.org/pdf/1704.04503.pdf&#34; target=&#34;_blank&#34;&gt;Improving Object Detection With One Line of Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&#34;https://arxiv.org/pdf/1809.08545.pdf&#34; target=&#34;_blank&#34;&gt;Bounding Box Regression with Uncertainty for Accurate Object Detection&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PR/ROC曲线及其相关性能评价指标</title>
      <link>https://YuxinZhaozyx.github.io/post/pr-roc/</link>
      <pubDate>Mon, 15 Jul 2019 21:05:58 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pr-roc/</guid>
      <description>

&lt;p&gt;在分类模型的评价标准中，PR曲线和ROC曲线被广泛应用于模型的性能评估。本文对PR曲线和ROC曲线及其相关的性能指标AUC, EER, AP, mAP, F1-measure进行介绍。&lt;/p&gt;

&lt;h2 id=&#34;混淆矩阵-confusion-matrix&#34;&gt;混淆矩阵 Confusion Matrix&lt;/h2&gt;

&lt;table align=&#34;center&#34; style= &#34;vertical-align: middle;&#34;&gt;
    &lt;tr&gt;
        &lt;th colspan=&#34;2&#34; rowspan=&#34;2&#34;&gt;&lt;/td&gt;
        &lt;th colspan=&#34;2&#34;&gt;Truth&lt;/td&gt;
        &lt;th rowspan=&#34;2&#34;&gt; $\sum$ &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;1&lt;/td&gt;
        &lt;th&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th rowspan=&#34;2&#34;&gt; Estimate &lt;/td&gt;
        &lt;th&gt; 1&lt;/td&gt;
        &lt;td&gt; TP &lt;/td&gt;
        &lt;td&gt; FP &lt;/td&gt;
        &lt;td&gt; TP+FP &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt; 0 &lt;/td&gt;
        &lt;td&gt; FN &lt;/td&gt;
        &lt;td&gt; TN &lt;/td&gt;
        &lt;td&gt; FN+TN &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th colspan=&#34;2&#34;&gt; $\sum$ &lt;/td&gt;
        &lt;td&gt; TP+FN &lt;/td&gt;
        &lt;td&gt; FP+TN &lt;/td&gt;
        &lt;td&gt; TP+TN+FP+FN &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;真正例 (True Positive, TP): 预测值和真实值都为1&lt;/li&gt;
&lt;li&gt;假正例 (False Positive, FP): 预测值为1，真实值都为0&lt;/li&gt;
&lt;li&gt;真反例 (True Negative, TN): 预测值和真实值都为0&lt;/li&gt;
&lt;li&gt;假反例 (False Negative, FN): 预测值为0，真实值都为1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由这四个指标衍生出的指标：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;查准率/准确率:                       $ \text{Precision} = \frac{TP}{TP+FP} $&lt;/li&gt;
&lt;li&gt;查全率/召回率:                       $\text{Recall} = \frac{TP}{TP+FN}$&lt;/li&gt;
&lt;li&gt;真阳率(True Positive Rate): $\text{TPR} = \frac{TP}{TP+FN}$&lt;/li&gt;
&lt;li&gt;假阳率(False Positive Rate): $\text{FPR} = \frac{FP}{FP+TN}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;roc曲线&#34;&gt;ROC曲线&lt;/h2&gt;

&lt;h3 id=&#34;roc定义&#34;&gt;ROC定义&lt;/h3&gt;

&lt;p&gt;ROC曲线(Receiver Operating Characteristic Curve, 受试者工作特征曲线)是比较分类模型好坏的可视化工具。&lt;/p&gt;

&lt;p&gt;以FPR为x轴，TPR为y轴绘制图。如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/ROC.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;roc曲线的衍生指标&#34;&gt;ROC曲线的衍生指标&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;EER(equal error rate): TPR=FPR时的值。&lt;/li&gt;
&lt;li&gt;AUC(area under curve): ROC曲线下的面积&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;pr曲线&#34;&gt;PR曲线&lt;/h2&gt;

&lt;h3 id=&#34;pr定义&#34;&gt;PR定义&lt;/h3&gt;

&lt;p&gt;PR曲线中P是Precision, R是Recall。&lt;/p&gt;

&lt;p&gt;以Recall为x轴，Precision为y轴。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/38381443447279.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;假设一次Object Detection的结果为：&lt;/strong&gt;（对于目标检测任务，当预测框与真实框&lt;a href=&#34;https://YuxinZhaozyx.github.io/post/what-is-iou&#34;&gt;IoU&lt;/a&gt;大于一定阈值时标记为TP；当预测框与真实框IoU小于一定阈值时标记为FP；一个真实框没有一个任何预测框与其重叠的为FN）
&lt;img src=&#34;image/samples_1_v2.png&#34; alt=&#34;img&#34; /&gt;
&lt;img src=&#34;image/table_1_v2.png&#34; alt=&#34;img&#34; /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对其confidence进行排序:&lt;/strong&gt;
&lt;img src=&#34;image/table_2_v2.png&#34; alt=&#34;img&#34; /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;根据上表的顺序绘制PR曲线：&lt;/strong&gt;
&lt;img src=&#34;image/precision_recall_example_1_v2.png&#34; alt=&#34;img&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;至此我们得到了一张PR图。&lt;/p&gt;

&lt;h3 id=&#34;pr图的衍生指标&#34;&gt;PR图的衍生指标&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;AP (Average Precision)&lt;/li&gt;
&lt;li&gt;mAP (mean Average Precision)&lt;/li&gt;
&lt;li&gt;F1-measure 综合评价指标&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面给出AP, mAP和F-measure的计算方法&lt;/p&gt;

&lt;h4 id=&#34;计算ap&#34;&gt;计算AP&lt;/h4&gt;

&lt;p&gt;AP 是针对某一类别进行计算的。&lt;/p&gt;

&lt;p&gt;下面我们先从图的角度来理解AP。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在2010年前，AP的计算方法是用11点插值法(11-point interpolation)&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;分别取 recall = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] 十一个点插值，对每一个插值recall取 recall&amp;rsquo; &amp;gt;= recall 的点中precision最大的值作为该插值recall对应的precision。&lt;/p&gt;

&lt;p&gt;计算公式为:
$$
P_{\text{interpolation}} (r) = \max_{r&amp;rsquo; \ge r}\left(P(r&amp;rsquo;)\right)
$$&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/11-pointInterpolation.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;计算这11个插值recall对应precision的均值即是AP。
$$
\begin{align}
AP &amp;amp;= \frac1{11} \sum_{r \in \{0,0.1,&amp;hellip;,1\}} P_{\text{interpolation}} (r) \\&lt;br /&gt;
&amp;amp;= \frac1{11} \left( 1 + 0.6666 + 0.4285 + 0.4285 + 0.4285 + 0 + 0 + 0 + 0 + 0 + 0  \right) \\&lt;br /&gt;
&amp;amp;= 26.84\%
\end{align}
$$
&lt;strong&gt;2010年后，AP的计算方法不再使用11点插值法，而是考虑所有的点&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;对所有Recall值，将Recall大于等于该Recall值的所有点中的最大precision作为该recall值对应的precision。公式仍是
$$
P_{\text{interpolation}} (r) = \max_{r&amp;rsquo; \ge r}\left(P(r&amp;rsquo;)\right)
$$
不同点在于AP的计算公式:
$$
AP = \int_0^1 P_{\text{interpolation}}(r) \, dr
$$
可以画出图帮助理解：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/interpolated_precision_v2.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/interpolated_precision-AUC_v2.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;按照上面的计算公式，则AP为：
$$
\begin{align}
A1 &amp;amp;= (0.0666 - 0) \times 1 = 0.0666 \\&lt;br /&gt;
A2 &amp;amp;= (0.1333-0.0666) \times 0.6666 = 0.04446222 \\&lt;br /&gt;
A3 &amp;amp;= (0.4-0.1333) \times 0.4285 = 0.11428095 \\&lt;br /&gt;
A4 &amp;amp;= (0.4666 - 0.4) \times 0.3043 = 0.02026638 \\&lt;br /&gt;
\\&lt;br /&gt;
AP &amp;amp;= A1+A2+A3+A4 \\&lt;br /&gt;
&amp;amp;= 0.0666 + 0.04446222 + 0.11428095 + 0.02026638 \\&lt;br /&gt;
&amp;amp;= 0.24560955 \\&lt;br /&gt;
&amp;amp;= 24.56\%
\end{align}
$$
以后的AP计算我会以2010年后的版本为准。&lt;/p&gt;

&lt;p&gt;接下来我将演示&lt;strong&gt;用表格来计算AP&lt;/strong&gt;，而不使用绘图的方式：&lt;/p&gt;

&lt;p&gt;按Confidence置信度来降序做出表格:&lt;/p&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Detection&lt;/th&gt;
        &lt;th&gt;Precision&lt;/th&gt;
        &lt;th&gt;Recall&lt;/th&gt;
        &lt;th&gt;Max Precision for Any Recall $r&#39; \ge r$&lt;/th&gt;
        &lt;th&gt;Average Precision&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;R&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td rowspan=&#34;2&#34;&gt;0.0666&lt;/td&gt;
        &lt;td rowspan=&#34;2&#34;&gt;1&lt;/td&gt;
        &lt;td rowspan=&#34;24&#34;&gt;24.56%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Y&lt;/td&gt;
        &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;J&lt;/td&gt;
        &lt;td&gt;0.6666&lt;/td&gt;
        &lt;td rowspan=&#34;7&#34;&gt;0.1333&lt;/td&gt;
        &lt;td rowspan=&#34;7&#34;&gt;0.6666&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;A&lt;/td&gt;
        &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;U&lt;/td&gt;
        &lt;td&gt;0.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;C&lt;/td&gt;
        &lt;td&gt;0.3333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;M&lt;/td&gt;
        &lt;td&gt;0.2857&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;F&lt;/td&gt;
        &lt;td&gt;0.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;D&lt;/td&gt;
        &lt;td&gt;0.2222&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;B&lt;/td&gt;
        &lt;td&gt;0.3&lt;/td&gt;
        &lt;td rowspan=&#34;2&#34;&gt;0.2&lt;/td&gt;
        &lt;td rowspan=&#34;13&#34;&gt;0.4285&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;H&lt;/td&gt;
        &lt;td&gt;0.2727&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;P&lt;/td&gt;
        &lt;td&gt;0.3333&lt;/td&gt;
        &lt;td&gt;0.2666&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;E&lt;/td&gt;
        &lt;td&gt;0.3846&lt;/td&gt;
        &lt;td&gt;0.3333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;X&lt;/td&gt;
        &lt;td&gt;0.4285&lt;/td&gt;
        &lt;td rowspan=&#34;9&#34;&gt;0.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;N&lt;/td&gt;
        &lt;td&gt;0.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;T&lt;/td&gt;
        &lt;td&gt;0.375&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;K&lt;/td&gt;
        &lt;td&gt;0.3529&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Q&lt;/td&gt;
        &lt;td&gt;0.3333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;V&lt;/td&gt;
        &lt;td&gt;0.3157&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;I&lt;/td&gt;
        &lt;td&gt;0.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;L&lt;/td&gt;
        &lt;td&gt;0.2857&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;S&lt;/td&gt;
        &lt;td&gt;0.2727&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;G&lt;/td&gt;
        &lt;td&gt;0.3043&lt;/td&gt;
        &lt;td rowspan=&#34;2&#34;&gt;0.4666&lt;/td&gt;
        &lt;td rowspan=&#34;2&#34;&gt;0.3043&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;O&lt;/td&gt;
        &lt;td&gt;0.2916&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;计算方法和原理同上，但画出表格可以直接计算AP:
$$
\begin{align}
AP &amp;amp;= 1 \times 0.0666 + 0.6666 \times (0.1333-0.0666) + 0.4285 \times(0.4-0.1333) + 0.3043 \times(0.4666 - 0.4) \\&lt;br /&gt;
&amp;amp;= 0.24560955
\end{align}
$$&lt;/p&gt;

&lt;h4 id=&#34;计算map&#34;&gt;计算mAP&lt;/h4&gt;

&lt;p&gt;mAP的全程是mean Average Precision，即是所有AP的均值。因为AP只是正对一类的准确率进行评估，而在多类别检测/分类任务中就需要一个指标来对整个多分类任务的性能进行评估，这就是mAP。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;计算公式&lt;/strong&gt;:
$$
mAP = \frac{\sum_c AP_c}{N_{class}}
$$&lt;/p&gt;

&lt;h4 id=&#34;计算f1-measure综合评价指标&#34;&gt;计算F1-measure综合评价指标&lt;/h4&gt;

&lt;p&gt;F-measure又称F-Score，是Precision和Recall的加权调和平均，常用于评价分类模型的好坏。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;计算公式&lt;/strong&gt;:
$$
F_\alpha = \frac{(\alpha^2+1)\text{Precision}\times\text{Recall}}{\alpha^2 \text{Precision} + \text{Recall}}
$$&lt;/p&gt;

&lt;p&gt;常见的&lt;strong&gt;F$_1$-measure&lt;/strong&gt;即为F-measure中$\alpha=1$的特例:
$$
F_1 = \frac{2 \times \text{Precision}\times\text{Recall}}{\text{Precision} + \text{Recall}}
$$&lt;/p&gt;

&lt;p&gt;该公式的另外一种形式可以帮助记忆:
$$
\frac2{F_1} = \frac1{\text{Precision}} + \frac1{\text{Recall}}
$$&lt;/p&gt;

&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;

&lt;p&gt;[1] &lt;a href=&#34;https://github.com/rafaelpadilla/Object-Detection-Metrics&#34; target=&#34;_blank&#34;&gt;rafaelpadilla/Object-Detection-Metrics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&#34;https://www.zhihu.com/question/53405779&#34; target=&#34;_blank&#34;&gt;目标检测中的mAP是什么含义?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&#34;http://blog.sina.com.cn/s/blog_9db078090102whzw.html&#34; target=&#34;_blank&#34;&gt;多标签图像分类任务的评价方法-mAP&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What is IoU?</title>
      <link>https://YuxinZhaozyx.github.io/post/what-is-iou/</link>
      <pubDate>Mon, 15 Jul 2019 20:20:32 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/what-is-iou/</guid>
      <description>&lt;p&gt;在目标检测任务中常常用IoU来计算预测窗口与真实窗口的交叠率。本文介绍IoU的概念。&lt;/p&gt;

&lt;p&gt;IoU (Intersection over Union, 交并比) 是两个窗口的交集与并集面积之比。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;区域 Region-1 与 Region-2 的交集如上图黄色区域&lt;/li&gt;
&lt;li&gt;区域 Region-1 与 Region-2 的并集如上图绿色区域&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;IoU计算公式&lt;/strong&gt;:
$$
IoU = \frac{\text{Region-1} \cap \text{Region-2}}{\text{Region-1} \cup \text{Region-2}}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What is RoI Pooling?</title>
      <link>https://YuxinZhaozyx.github.io/post/what-is-roi-pooling/</link>
      <pubDate>Mon, 15 Jul 2019 11:52:13 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/what-is-roi-pooling/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://YuxinZhaozyx.github.io/post/what-is-roi/&#34;&gt;上文&lt;/a&gt;讲了什么是RoI(Region of Interest, 感兴趣区域)，本文讲述RoI Pooling的概念。&lt;/p&gt;

&lt;h3 id=&#34;roi-pooling-的作用&#34;&gt;RoI Pooling 的作用&lt;/h3&gt;

&lt;p&gt;RoI Pooling是Pooling(池化)的一种，而且是针对RoI的池化。它的作用是&lt;strong&gt;输入尺寸不固定的特征图，输出尺寸固定的特征图&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&#34;roi-pooling-的输入&#34;&gt;RoI Pooling 的输入&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特征图 feature map&lt;/strong&gt;：由原图像通过CNN计算得到的特征图&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;感兴趣区域 RoI&lt;/strong&gt;: 许多候选框，形状为$1 \times 5 \times 1$ (4 个坐标[x,y,w,h] + 索引[index])&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;RoI的坐标的参考系是原图(CNN的输入)&lt;/strong&gt;，而不是feature map。
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;roi-pooling-的输出&#34;&gt;RoI Pooling 的输出&lt;/h3&gt;

&lt;p&gt;输出batch个张量，其中batch的值等于RoI的个数，张量大小为(channel, w, h)， w和h人为指定。RoI Pooling的过程就是将一个个尺寸不同的RoI都映射成大小固定的(w, h)的RoI。&lt;/p&gt;

&lt;h2 id=&#34;图解-roi-pooling&#34;&gt;图解 RoI Pooling&lt;/h2&gt;

&lt;p&gt;考虑一个 $8 \times 8$ 大小的feature map，一个RoI，以及输出大小为 $2 \times 2$.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;输入固定大小的feature map&lt;/strong&gt;
&lt;img src=&#34;image/input-feature-map.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;region proposal 投影后的位置&lt;/strong&gt;
&lt;img src=&#34;image/region-proposal.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;将其划分为 $(2 \times 2)$ 个sections (因为输出大小为$2 \times 2$)&lt;/strong&gt;
&lt;img src=&#34;image/max-pooling-sections.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对每个section做max pooling，可以得到:&lt;/strong&gt;
&lt;img src=&#34;image/roi-pooling-result.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;参考引用&#34;&gt;参考引用&lt;/h2&gt;

&lt;p&gt;[1] &lt;a href=&#34;https://deepsense.ai/region-of-interest-pooling-explained/&#34; target=&#34;_blank&#34;&gt;Region of interest pooling explained&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&#34;https://blog.csdn.net/lanran2/article/details/60143861&#34; target=&#34;_blank&#34;&gt;ROI Pooling层解析&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What is RoI?</title>
      <link>https://YuxinZhaozyx.github.io/post/what-is-roi/</link>
      <pubDate>Mon, 15 Jul 2019 11:07:26 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/what-is-roi/</guid>
      <description>

&lt;h2 id=&#34;roi概念&#34;&gt;RoI概念&lt;/h2&gt;

&lt;p&gt;RoI的全称是Region of Interest，中文名称是&amp;rdquo;感兴趣区域&amp;rdquo;。&lt;/p&gt;

&lt;p&gt;RoI是从图像中选择的一个图像区域，这个区域是你进行图像分析的重点。圈出这块区域可以得到一个子图像(subimage)，之后就可以在这个区域内使用进行进一步处理。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;目的：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;减少处理时间&lt;/li&gt;
&lt;li&gt;增加精度&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
