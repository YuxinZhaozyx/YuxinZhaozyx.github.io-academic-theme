<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep-learning on YuxinZhao</title>
    <link>https://YuxinZhaozyx.github.io/categories/deep-learning/</link>
    <description>Recent content in deep-learning on YuxinZhao</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year} YuxinZhao</copyright>
    <lastBuildDate>Mon, 22 Jul 2019 20:48:56 +0800</lastBuildDate>
    
	    <atom:link href="https://YuxinZhaozyx.github.io/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PyTorch Learning Note-3</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-3/</link>
      <pubDate>Mon, 22 Jul 2019 20:48:56 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-3/</guid>
      <description>

&lt;p&gt;使用torch.nn包来构建神经网络。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nn&lt;/code&gt;包依赖&lt;code&gt;autograd&lt;/code&gt;包来定义模型并求导。 一个&lt;code&gt;nn.Module&lt;/code&gt;包含各个层和一个&lt;code&gt;forward(input)&lt;/code&gt;方法，该方法返回&lt;code&gt;output&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/68747470733a2f2f7079746f7263682e6f72672f7475746f7269616c732f5f696d616765732f6d6e6973742e706e67.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。&lt;/p&gt;

&lt;p&gt;神经网络的典型训练过程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;定义包含一些可学习的参数(或者叫权重)神经网络模型；&lt;/li&gt;
&lt;li&gt;在数据集上迭代；&lt;/li&gt;
&lt;li&gt;通过神经网络处理输入；&lt;/li&gt;
&lt;li&gt;计算损失(输出结果和正确值的差值大小)；&lt;/li&gt;
&lt;li&gt;将梯度反向传播回网络的参数；&lt;/li&gt;
&lt;li&gt;更新网络的参数，主要使用如下简单的更新原则： &lt;code&gt;weight = weight - learning_rate * gradient&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;定义网络&#34;&gt;定义网络&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torch.nn as nn
import torch.nn.functional as F 

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()

        # 1 input image channel
        # 6 output image channel
        # 5x5 square convolution kernel
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)

        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # if the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)

        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        
        num_features = 1
        for s in size:
            num_features *= s
        return num_features
    
net = Net()
print(net)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;在模型中必须要定义 &lt;code&gt;forward&lt;/code&gt; 函数&lt;/strong&gt;，&lt;code&gt;backward&lt;/code&gt; 函数（用来计算梯度）会被&lt;code&gt;autograd&lt;/code&gt;自动创建。 可以在 &lt;code&gt;forward&lt;/code&gt; 函数中使用任何针对 Tensor 的操作。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;net.parameters()&lt;/code&gt;返回可被学习的参数（权重）列表和值&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;params = list(net.parameters())
print(len(params))
print(params[0].size())  # params[0] == net.conv1.weight
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;10
torch.Size([6, 1, 5, 5]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试随机输入32×32。 注：这个网络（LeNet）期望的输入大小是32×32，如果使用MNIST数据集来训练这个网络，请把图片大小重新调整到32×32。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input = torch.randn(1, 1, 32, 32)
out = net(input)
print(out)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[ 0.1052, -0.0361,  0.1122,  0.1072,  0.0887,  0.0477,
 0.0916, -0.0594,
         -0.1450,  0.0574]], grad_fn=&amp;lt;AddmmBackward&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将所有参数的梯度缓存清零，然后进行随机梯度的的反向传播：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;net.zero_grad()
out.backward(torch.randn(1, 10))
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;torch.nn&lt;/code&gt; 只支持小批量输入。整个 &lt;code&gt;torch.nn&lt;/code&gt; 包都只支持小批量样本，而不支持单个样本。 例如，&lt;code&gt;nn.Conv2d&lt;/code&gt; 接受一个4维的张量， &lt;code&gt;每一维分别是sSamples * nChannels * Height * Width（样本数*通道数*高*宽）&lt;/code&gt;。 &lt;strong&gt;如果你有单个样本，只需使用 &lt;code&gt;input.unsqueeze(0)&lt;/code&gt; 来添加其它的维数&lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;我们回顾一下到目前为止用到的类。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;回顾:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;：一个用过自动调用 &lt;code&gt;backward()&lt;/code&gt;实现支持自动梯度计算的 &lt;em&gt;多维数组&lt;/em&gt; ， 并且保存关于这个向量的梯度&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn.Module&lt;/code&gt;：神经网络模块。封装参数、移动到GPU上运行、导出、加载等。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn.Parameter&lt;/code&gt;：一种变量，当把它赋值给一个&lt;code&gt;Module&lt;/code&gt;时，被 自动地注册为一个参数。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;autograd.Function&lt;/code&gt;：实现一个自动求导操作的前向和反向定义，每个变量操作至少创建一个函数节点，每一个&lt;code&gt;Tensor&lt;/code&gt;的操作都h会创建一个接到创建&lt;code&gt;Tensor&lt;/code&gt;和 编码其历史 的函数的&lt;code&gt;Function&lt;/code&gt;节点。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;损失函数&#34;&gt;损失函数&lt;/h2&gt;

&lt;p&gt;一个损失函数接受一对 (output, target) 作为输入，计算一个值来估计网络的输出和目标值相差多少。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://pytorch.org/docs/nn&#34; target=&#34;_blank&#34;&gt;nn包&lt;/a&gt;中有很多不同的&lt;a href=&#34;https://pytorch.org/docs/nn.html#loss-functions&#34; target=&#34;_blank&#34;&gt;损失函数&lt;/a&gt;。 &lt;code&gt;nn.MSELoss&lt;/code&gt;是一个比较简单的损失函数，它计算输出和目标间的&lt;strong&gt;均方误差&lt;/strong&gt;， 例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input = torch.randn(1, 1, 32, 32)
output = net(input)
target = torch.randn(10)  # 随机值作为样例
target = target.view(1, -1)  # 使target和output的shape相同

criterion = nn.MSELoss()
loss = criterion(output, target)
print(loss)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor(1.1509, grad_fn=&amp;lt;MseLossBackward&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;反向传播&#34;&gt;反向传播&lt;/h2&gt;

&lt;p&gt;调用&lt;code&gt;loss.backward()&lt;/code&gt;获得反向传播的误差。&lt;/p&gt;

&lt;p&gt;但是在调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。&lt;/p&gt;

&lt;p&gt;现在，我们将调用&lt;code&gt;loss.backward()&lt;/code&gt;，并查看conv1层的偏差（bias）项在反向传播前后的梯度。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;net.zero_grad()  # 清除梯度

print(&#39;conv1.bias.grad before backward&#39;)
print(net.conv1.bias.grad)

loss.backward()

print(&#39;conv1.bias.grad after backward&#39;)
print(net.conv1.bias.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conv1.bias.grad before backward
None
conv1.bias.grad after backward
tensor([ 0.0027, -0.0142,  0.0197,  0.0021, -0.0018,  0.0001])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;更新权重&#34;&gt;更新权重&lt;/h2&gt;

&lt;p&gt;在实践中最简单的权重更新规则是随机梯度下降（SGD）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; weight = weight - learning_rate * gradient
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以使用简单的Python代码实现这个规则：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包&lt;code&gt;torch.optim&lt;/code&gt;实现了所有的这些规则。 使用它们非常简单：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01)
criterion = nn.MSELoss()

# in your training loop
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()   # Does the update
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning Note-2</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-2/</link>
      <pubDate>Mon, 22 Jul 2019 19:20:28 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-2/</guid>
      <description>

&lt;h1 id=&#34;autograd-自动求导机制&#34;&gt;Autograd: 自动求导机制&lt;/h1&gt;

&lt;p&gt;PyTorch 中所有神经网络的核心是 &lt;code&gt;autograd&lt;/code&gt; 包。 我们先简单介绍一下这个包，然后训练第一个简单的神经网络。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;autograd&lt;/code&gt;包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。&lt;/p&gt;

&lt;h2 id=&#34;张量-tensor&#34;&gt;张量 Tensor&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;是这个包的核心类。如果设置 &lt;code&gt;.requires_grad&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt;，那么将会追踪所有对于该张量的操作。 当完成计算后通过调用 &lt;code&gt;.backward()&lt;/code&gt;，自动计算所有的梯度， 这个张量的所有梯度将会自动积累到 &lt;code&gt;.grad&lt;/code&gt; 属性。&lt;/p&gt;

&lt;p&gt;要阻止张量跟踪历史记录，可以调用&lt;code&gt;.detach()&lt;/code&gt;方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。&lt;/p&gt;

&lt;p&gt;为了防止跟踪历史记录（和使用内存），可以将代码块包装在&lt;code&gt;with torch.no_grad()：&lt;/code&gt;中。 在评估模型时特别有用，因为模型可能具有&lt;code&gt;requires_grad = True&lt;/code&gt;的可训练参数，但是我们不需要梯度计算。&lt;/p&gt;

&lt;p&gt;在自动梯度计算中还有另外一个重要的类&lt;code&gt;Function&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Tensor&lt;/code&gt; 和 &lt;code&gt;Function&lt;/code&gt;互相连接并生成一个非循环图，它表示和存储了完整的计算历史。 每个张量都有一个&lt;code&gt;.grad_fn&lt;/code&gt;属性，这个属性引用了一个创建了&lt;code&gt;Tensor&lt;/code&gt;的&lt;code&gt;Function&lt;/code&gt;（除非这个张量是用户手动创建的，即，这个张量的 &lt;code&gt;grad_fn&lt;/code&gt; 是 &lt;code&gt;None&lt;/code&gt;）。&lt;/p&gt;

&lt;p&gt;如果需要计算导数，你可以在&lt;code&gt;Tensor&lt;/code&gt;上调用&lt;code&gt;.backward()&lt;/code&gt;。 如果&lt;code&gt;Tensor&lt;/code&gt;是一个标量（即它包含一个元素数据）则不需要为&lt;code&gt;backward()&lt;/code&gt;指定任何参数， 但是如果它有更多的元素，你需要指定一个&lt;code&gt;gradient&lt;/code&gt; 参数来匹配张量的形状。&lt;/p&gt;

&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    在其他的文章中你可能会看到说将Tensor包裹到Variable中提供自动梯度计算，Variable 这个在0.41版中已经被标注为过期了，现在可以直接使用Tensor，官方文档在&lt;a href=&#34;https://pytorch.org/docs/stable/autograd.html#variable-deprecated&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;创建一个张量并设置 &lt;code&gt;requires_grad=True&lt;/code&gt; 用来追踪他的计算历史&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.ones(2, 2, requires_grad=True)
print(x)

y = x + 2  # 对x进行操作
print(y)
print(&amp;quot;y.grad_fn: &amp;quot;, y.grad_fn)  # 结果y已经被计算出来了，所以，grad_fn已经被自动生成了

z = y * y * 3  # 对y进行操作
out = z.mean()
print(z)
print(out)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
tensor([[3., 3.],
        [3., 3.]], grad_fn=&amp;lt;AddBackward0&amp;gt;)
y.grad_fn:  &amp;lt;AddBackward0 object at 0x00000202022DAE48&amp;gt;
tensor([[27., 27.],
        [27., 27.]], grad_fn=&amp;lt;MulBackward0&amp;gt;)
tensor(27., grad_fn=&amp;lt;MeanBackward0&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;.requires_grad_( ... )&lt;/code&gt; 可以改变现有张量的 &lt;code&gt;requires_grad&lt;/code&gt;属性。 如果没有指定的话，默认输入的flag是 &lt;code&gt;False&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = torch.ones(2, 2)
a = (a * 3) / (a - 1)
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;False
True
&amp;lt;SumBackward0 object at 0x0000028141CBADA0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;梯度&#34;&gt;梯度&lt;/h2&gt;

&lt;p&gt;反向传播 因为 &lt;code&gt;out&lt;/code&gt;是一个纯量（scalar），&lt;code&gt;out.backward()&lt;/code&gt; 等于&lt;code&gt;out.backward(torch.tensor(1))&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;out.backward()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;print gradients $\frac{d(out)}{dx}$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.ones(2, 2, requires_grad=True)
y = x + 2  
z = y * y * 3  
out = z.mean()

out.backward()
print(x.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以用自动求导做更多操作&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn(3, requires_grad=True)

y = x * 2
while y.data.norm() &amp;lt; 1000:
    y = y * 2

print(y)
gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
y.backward(gradients)

print(x.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([-790.8533,  793.1236,  307.1018], grad_fn=&amp;lt;MulBackward0&amp;gt;)
tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果&lt;code&gt;.requires_grad=True&lt;/code&gt;但是你又不希望进行autograd的计算， 那么可以将变量包裹在 &lt;code&gt;with torch.no_grad()&lt;/code&gt;中:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print(x.requires_grad)
    print((x ** 2).requires_grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;True
True
True
False
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;更多阅读：&lt;/strong&gt; autograd 和 Function 的&lt;a href=&#34;https://pytorch.org/docs/autograd&#34; target=&#34;_blank&#34;&gt;官方文档&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning Note-1</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-1/</link>
      <pubDate>Mon, 22 Jul 2019 18:11:52 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-1/</guid>
      <description>

&lt;h2 id=&#34;pytorch-是什么&#34;&gt;PyTorch 是什么?&lt;/h2&gt;

&lt;p&gt;基于Python的科学计算包，服务于以下两种场景:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;作为NumPy的替代品，可以使用GPU的强大计算能力&lt;/li&gt;
&lt;li&gt;提供最大的灵活性和高速的深度学习研究平台&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tensor-张量&#34;&gt;Tensor 张量&lt;/h2&gt;

&lt;p&gt;Tensors与Numpy中的 ndarrays类似，但是在PyTorch中 Tensors 可以使用GPU进行计算.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from __future__ import print_function
import torch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建一个  5x5 的矩阵，但未初始化:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.empty(5, 3)
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[9.5511e-39, 1.0102e-38, 4.6837e-39],
        [4.9592e-39, 5.0510e-39, 9.9184e-39],
        [9.0000e-39, 1.0561e-38, 1.0653e-38],
        [4.1327e-39, 8.9082e-39, 9.8265e-39],
        [9.4592e-39, 1.0561e-38, 1.0653e-38]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建一个随机初始化的矩阵:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.rand(5, 3)
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[0.6004, 0.9095, 0.5525],
        [0.2870, 0.2680, 0.1937],
        [0.9153, 0.0150, 0.5165],
        [0.7875, 0.7397, 0.9305],
        [0.8575, 0.1453, 0.2655]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建一个0填充的矩阵，数据类型为&lt;code&gt;long&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.zeros(5, 3, dtype=torch.long)
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建tensor并使用现有数据初始化:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.tensor([5.5, 3])
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([5.5000, 3.0000])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;根据现有的张量创建张量&lt;/strong&gt;。 这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.tensor([5.5, 3])
print(x)

x = x.new_ones(5, 3, dtype=torch.double)      # new_* 方法来创建对象
print(x)

x = torch.randn_like(x, dtype=torch.float)    # 覆盖 dtype!
print(x)                                      #  对象的size 是相同的，只是值和类型发生了变化
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([5.5000, 3.0000])
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[-0.5648,  1.4639, -0.1247],
        [ 0.4187,  0.0255, -0.0938],
        [-1.2237,  0.3889,  0.9847],
        [-0.2423, -3.3706, -0.3511],
        [-1.1498, -1.1044,  0.4582]])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;获取-size&#34;&gt;获取 size&lt;/h2&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    使用&lt;code&gt;size&lt;/code&gt;方法与Numpy的&lt;code&gt;shape&lt;/code&gt;属性返回的相同，张量也支持&lt;code&gt;shape&lt;/code&gt;属性
  &lt;/div&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.ones(5, 3)
print(x)

print(&amp;quot;x.size(): &amp;quot;, x.size())
print(&amp;quot;x.shape:  &amp;quot;, x.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
x.size():  torch.Size([5, 3])
x.shape:   torch.Size([5, 3])
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;torch.Size()&lt;/code&gt;返回值是&lt;code&gt;tuple&lt;/code&gt;类型，所以它支持&lt;code&gt;tuple&lt;/code&gt;类型的所有操作
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;operation-操作&#34;&gt;Operation 操作&lt;/h2&gt;

&lt;h3 id=&#34;加法&#34;&gt;加法&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.rand(5, 3)
y = torch.rand(5, 3)

sum = x + y                 # 加法1，操作符
sum = torch.add(x, y)       # 加法2，函数
torch.add(x, y, out=sum)    # 加法3，提供输出张量sum作为参数
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;替换&#34;&gt;替换&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# add x to y
y.add_(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    任何 以&lt;code&gt;_&lt;/code&gt; 结尾的操作都会用结果替换原变量. 例如: &lt;code&gt;x.copy_(y)&lt;/code&gt;, &lt;code&gt;x.t_()&lt;/code&gt;, 都会改变 &lt;code&gt;x&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;截取&#34;&gt;截取&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x[:, 1])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;view-reshape&#34;&gt;view / reshape&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;torch.view&lt;/code&gt; 可以改变张量的维度和大小，与numpy的reshape类似&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  #  size -1 从其他维度推断
print(x.size(), y.size(), z.size())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;只有一个元素的张量取值&#34;&gt;只有一个元素的张量取值&lt;/h3&gt;

&lt;p&gt;如果你有只有一个元素的张量，使用&lt;code&gt;.item()&lt;/code&gt;来得到Python数据类型的数值&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn(1)
print(x)
print(x.item())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([-0.2036])
-0.203627809882164
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    更多操作&lt;a href=&#34;https://pytorch.org/docs/stable/torch.html&#34; target=&#34;_blank&#34;&gt;点击此处&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;numpy-转换&#34;&gt;Numpy 转换&lt;/h2&gt;

&lt;p&gt;Torch Tensor与NumPy数组&lt;strong&gt;共享底层内存地址&lt;/strong&gt;，修改一个会导致另一个的变化。&lt;/p&gt;

&lt;h3 id=&#34;torch-tensor-转换成-numpy数组&#34;&gt;Torch Tensor 转换成 NumPy数组&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = torch.ones(5)
print(a)

b = a.numpy()
print(b)

a.add_(1)
print(a)
print(b)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([1., 1., 1., 1., 1.])
[1. 1. 1. 1. 1.]
tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;numpy数组-转换成-torch-tensor&#34;&gt;NumPy数组 转换成 Torch Tensor&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = np.ones(5)
b = torch.from_numpy(a)
print(a)
print(b)

np.add(a, 1, out=a)
print(a)
print(b)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[1. 1. 1. 1. 1.]
tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    所有的 &lt;code&gt;Tensor&lt;/code&gt; 类型默认都是基于CPU， &lt;code&gt;CharTensor&lt;/code&gt; 类型不支持到 NumPy 的转换.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;cuda-张量&#34;&gt;CUDA 张量&lt;/h2&gt;

&lt;p&gt;使用&lt;code&gt;.to&lt;/code&gt; 方法 可以将Tensor移动到任何设备中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.rand(1)

# is_available 函数判断是否有cuda可以使用
# torch.device 将张量移动到指定的设备中
if torch.cuda.is_available():
    device = torch.device(&amp;quot;cuda&amp;quot;)          # a CUDA 设备对象
    y = torch.ones_like(x, device=device)  # 直接从GPU创建张量
    x = x.to(device)                       # 或者直接使用 .to(&amp;quot;cuda&amp;quot;) 将张量移动到cuda中
    z = x + y
    print(z)
    print(z.to(&amp;quot;cpu&amp;quot;, torch.double))       # .to 也会对变量的类型做更改
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([1.2840], device=&#39;cuda:0&#39;)
tensor([1.2840], dtype=torch.float64)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning</title>
      <link>https://YuxinZhaozyx.github.io/project/pytorch-learning/</link>
      <pubDate>Mon, 22 Jul 2019 17:37:53 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/project/pytorch-learning/</guid>
      <description>

&lt;p&gt;This project is to record my learning road of pytorch framework.&lt;/p&gt;

&lt;h2 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34; target=&#34;_blank&#34;&gt;官方教程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/zergtant/pytorch-handbook&#34; target=&#34;_blank&#34;&gt;PyTorch中文手册&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
