<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep-learning on YuxinZhao</title>
    <link>https://YuxinZhaozyx.github.io/categories/deep-learning/</link>
    <description>Recent content in deep-learning on YuxinZhao</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year} YuxinZhao</copyright>
    <lastBuildDate>Mon, 22 Jul 2019 19:20:28 +0800</lastBuildDate>
    
	    <atom:link href="https://YuxinZhaozyx.github.io/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PyTorch Learning Note-2</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-2/</link>
      <pubDate>Mon, 22 Jul 2019 19:20:28 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-2/</guid>
      <description>

&lt;h1 id=&#34;autograd-自动求导机制&#34;&gt;Autograd: 自动求导机制&lt;/h1&gt;

&lt;p&gt;PyTorch 中所有神经网络的核心是 &lt;code&gt;autograd&lt;/code&gt; 包。 我们先简单介绍一下这个包，然后训练第一个简单的神经网络。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;autograd&lt;/code&gt;包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。&lt;/p&gt;

&lt;h2 id=&#34;张量-tensor&#34;&gt;张量 Tensor&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;是这个包的核心类。如果设置 &lt;code&gt;.requires_grad&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt;，那么将会追踪所有对于该张量的操作。 当完成计算后通过调用 &lt;code&gt;.backward()&lt;/code&gt;，自动计算所有的梯度， 这个张量的所有梯度将会自动积累到 &lt;code&gt;.grad&lt;/code&gt; 属性。&lt;/p&gt;

&lt;p&gt;要阻止张量跟踪历史记录，可以调用&lt;code&gt;.detach()&lt;/code&gt;方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。&lt;/p&gt;

&lt;p&gt;为了防止跟踪历史记录（和使用内存），可以将代码块包装在&lt;code&gt;with torch.no_grad()：&lt;/code&gt;中。 在评估模型时特别有用，因为模型可能具有&lt;code&gt;requires_grad = True&lt;/code&gt;的可训练参数，但是我们不需要梯度计算。&lt;/p&gt;

&lt;p&gt;在自动梯度计算中还有另外一个重要的类&lt;code&gt;Function&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Tensor&lt;/code&gt; 和 &lt;code&gt;Function&lt;/code&gt;互相连接并生成一个非循环图，它表示和存储了完整的计算历史。 每个张量都有一个&lt;code&gt;.grad_fn&lt;/code&gt;属性，这个属性引用了一个创建了&lt;code&gt;Tensor&lt;/code&gt;的&lt;code&gt;Function&lt;/code&gt;（除非这个张量是用户手动创建的，即，这个张量的 &lt;code&gt;grad_fn&lt;/code&gt; 是 &lt;code&gt;None&lt;/code&gt;）。&lt;/p&gt;

&lt;p&gt;如果需要计算导数，你可以在&lt;code&gt;Tensor&lt;/code&gt;上调用&lt;code&gt;.backward()&lt;/code&gt;。 如果&lt;code&gt;Tensor&lt;/code&gt;是一个标量（即它包含一个元素数据）则不需要为&lt;code&gt;backward()&lt;/code&gt;指定任何参数， 但是如果它有更多的元素，你需要指定一个&lt;code&gt;gradient&lt;/code&gt; 参数来匹配张量的形状。&lt;/p&gt;

&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    在其他的文章中你可能会看到说将Tensor包裹到Variable中提供自动梯度计算，Variable 这个在0.41版中已经被标注为过期了，现在可以直接使用Tensor，官方文档在&lt;a href=&#34;https://pytorch.org/docs/stable/autograd.html#variable-deprecated&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;创建一个张量并设置 &lt;code&gt;requires_grad=True&lt;/code&gt; 用来追踪他的计算历史&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.ones(2, 2, requires_grad=True)
print(x)

y = x + 2  # 对x进行操作
print(y)
print(&amp;quot;y.grad_fn: &amp;quot;, y.grad_fn)  # 结果y已经被计算出来了，所以，grad_fn已经被自动生成了

z = y * y * 3  # 对y进行操作
out = z.mean()
print(z)
print(out)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
tensor([[3., 3.],
        [3., 3.]], grad_fn=&amp;lt;AddBackward0&amp;gt;)
y.grad_fn:  &amp;lt;AddBackward0 object at 0x00000202022DAE48&amp;gt;
tensor([[27., 27.],
        [27., 27.]], grad_fn=&amp;lt;MulBackward0&amp;gt;)
tensor(27., grad_fn=&amp;lt;MeanBackward0&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;.requires_grad_( ... )&lt;/code&gt; 可以改变现有张量的 &lt;code&gt;requires_grad&lt;/code&gt;属性。 如果没有指定的话，默认输入的flag是 &lt;code&gt;False&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = torch.ones(2, 2)
a = (a * 3) / (a - 1)
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;False
True
&amp;lt;SumBackward0 object at 0x0000028141CBADA0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;梯度&#34;&gt;梯度&lt;/h2&gt;

&lt;p&gt;反向传播 因为 &lt;code&gt;out&lt;/code&gt;是一个纯量（scalar），&lt;code&gt;out.backward()&lt;/code&gt; 等于&lt;code&gt;out.backward(torch.tensor(1))&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;out.backward()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;print gradients $\frac{d(out)}{dx}$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.ones(2, 2, requires_grad=True)
y = x + 2  
z = y * y * 3  
out = z.mean()

out.backward()
print(x.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以用自动求导做更多操作&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn(3, requires_grad=True)

y = x * 2
while y.data.norm() &amp;lt; 1000:
    y = y * 2

print(y)
gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
y.backward(gradients)

print(x.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([-790.8533,  793.1236,  307.1018], grad_fn=&amp;lt;MulBackward0&amp;gt;)
tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果&lt;code&gt;.requires_grad=True&lt;/code&gt;但是你又不希望进行autograd的计算， 那么可以将变量包裹在 &lt;code&gt;with torch.no_grad()&lt;/code&gt;中:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print(x.requires_grad)
    print((x ** 2).requires_grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;True
True
True
False
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;更多阅读：&lt;/strong&gt; autograd 和 Function 的&lt;a href=&#34;https://pytorch.org/docs/autograd&#34; target=&#34;_blank&#34;&gt;官方文档&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning Note-1</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-1/</link>
      <pubDate>Mon, 22 Jul 2019 18:11:52 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-1/</guid>
      <description>

&lt;h2 id=&#34;pytorch-是什么&#34;&gt;PyTorch 是什么?&lt;/h2&gt;

&lt;p&gt;基于Python的科学计算包，服务于以下两种场景:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;作为NumPy的替代品，可以使用GPU的强大计算能力&lt;/li&gt;
&lt;li&gt;提供最大的灵活性和高速的深度学习研究平台&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tensor-张量&#34;&gt;Tensor 张量&lt;/h2&gt;

&lt;p&gt;Tensors与Numpy中的 ndarrays类似，但是在PyTorch中 Tensors 可以使用GPU进行计算.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from __future__ import print_function
import torch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建一个  5x5 的矩阵，但未初始化:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.empty(5, 3)
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[9.5511e-39, 1.0102e-38, 4.6837e-39],
        [4.9592e-39, 5.0510e-39, 9.9184e-39],
        [9.0000e-39, 1.0561e-38, 1.0653e-38],
        [4.1327e-39, 8.9082e-39, 9.8265e-39],
        [9.4592e-39, 1.0561e-38, 1.0653e-38]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建一个随机初始化的矩阵:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.rand(5, 3)
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[0.6004, 0.9095, 0.5525],
        [0.2870, 0.2680, 0.1937],
        [0.9153, 0.0150, 0.5165],
        [0.7875, 0.7397, 0.9305],
        [0.8575, 0.1453, 0.2655]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建一个0填充的矩阵，数据类型为&lt;code&gt;long&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.zeros(5, 3, dtype=torch.long)
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建tensor并使用现有数据初始化:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.tensor([5.5, 3])
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([5.5000, 3.0000])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;根据现有的张量创建张量&lt;/strong&gt;。 这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.tensor([5.5, 3])
print(x)

x = x.new_ones(5, 3, dtype=torch.double)      # new_* 方法来创建对象
print(x)

x = torch.randn_like(x, dtype=torch.float)    # 覆盖 dtype!
print(x)                                      #  对象的size 是相同的，只是值和类型发生了变化
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([5.5000, 3.0000])
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[-0.5648,  1.4639, -0.1247],
        [ 0.4187,  0.0255, -0.0938],
        [-1.2237,  0.3889,  0.9847],
        [-0.2423, -3.3706, -0.3511],
        [-1.1498, -1.1044,  0.4582]])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;获取-size&#34;&gt;获取 size&lt;/h2&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    使用&lt;code&gt;size&lt;/code&gt;方法与Numpy的&lt;code&gt;shape&lt;/code&gt;属性返回的相同，张量也支持&lt;code&gt;shape&lt;/code&gt;属性
  &lt;/div&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.ones(5, 3)
print(x)

print(&amp;quot;x.size(): &amp;quot;, x.size())
print(&amp;quot;x.shape:  &amp;quot;, x.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
x.size():  torch.Size([5, 3])
x.shape:   torch.Size([5, 3])
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;torch.Size()&lt;/code&gt;返回值是&lt;code&gt;tuple&lt;/code&gt;类型，所以它支持&lt;code&gt;tuple&lt;/code&gt;类型的所有操作
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;operation-操作&#34;&gt;Operation 操作&lt;/h2&gt;

&lt;h3 id=&#34;加法&#34;&gt;加法&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.rand(5, 3)
y = torch.rand(5, 3)

sum = x + y                 # 加法1，操作符
sum = torch.add(x, y)       # 加法2，函数
torch.add(x, y, out=sum)    # 加法3，提供输出张量sum作为参数
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;替换&#34;&gt;替换&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# add x to y
y.add_(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    任何 以&lt;code&gt;_&lt;/code&gt; 结尾的操作都会用结果替换原变量. 例如: &lt;code&gt;x.copy_(y)&lt;/code&gt;, &lt;code&gt;x.t_()&lt;/code&gt;, 都会改变 &lt;code&gt;x&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;截取&#34;&gt;截取&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x[:, 1])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;view-reshape&#34;&gt;view / reshape&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;torch.view&lt;/code&gt; 可以改变张量的维度和大小，与numpy的reshape类似&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  #  size -1 从其他维度推断
print(x.size(), y.size(), z.size())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;只有一个元素的张量取值&#34;&gt;只有一个元素的张量取值&lt;/h3&gt;

&lt;p&gt;如果你有只有一个元素的张量，使用&lt;code&gt;.item()&lt;/code&gt;来得到Python数据类型的数值&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn(1)
print(x)
print(x.item())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([-0.2036])
-0.203627809882164
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    更多操作&lt;a href=&#34;https://pytorch.org/docs/stable/torch.html&#34; target=&#34;_blank&#34;&gt;点击此处&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;numpy-转换&#34;&gt;Numpy 转换&lt;/h2&gt;

&lt;p&gt;Torch Tensor与NumPy数组&lt;strong&gt;共享底层内存地址&lt;/strong&gt;，修改一个会导致另一个的变化。&lt;/p&gt;

&lt;h3 id=&#34;torch-tensor-转换成-numpy数组&#34;&gt;Torch Tensor 转换成 NumPy数组&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = torch.ones(5)
print(a)

b = a.numpy()
print(b)

a.add_(1)
print(a)
print(b)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([1., 1., 1., 1., 1.])
[1. 1. 1. 1. 1.]
tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;numpy数组-转换成-torch-tensor&#34;&gt;NumPy数组 转换成 Torch Tensor&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = np.ones(5)
b = torch.from_numpy(a)
print(a)
print(b)

np.add(a, 1, out=a)
print(a)
print(b)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[1. 1. 1. 1. 1.]
tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    所有的 &lt;code&gt;Tensor&lt;/code&gt; 类型默认都是基于CPU， &lt;code&gt;CharTensor&lt;/code&gt; 类型不支持到 NumPy 的转换.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;cuda-张量&#34;&gt;CUDA 张量&lt;/h2&gt;

&lt;p&gt;使用&lt;code&gt;.to&lt;/code&gt; 方法 可以将Tensor移动到任何设备中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.rand(1)

# is_available 函数判断是否有cuda可以使用
# torch.device 将张量移动到指定的设备中
if torch.cuda.is_available():
    device = torch.device(&amp;quot;cuda&amp;quot;)          # a CUDA 设备对象
    y = torch.ones_like(x, device=device)  # 直接从GPU创建张量
    x = x.to(device)                       # 或者直接使用 .to(&amp;quot;cuda&amp;quot;) 将张量移动到cuda中
    z = x + y
    print(z)
    print(z.to(&amp;quot;cpu&amp;quot;, torch.double))       # .to 也会对变量的类型做更改
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([1.2840], device=&#39;cuda:0&#39;)
tensor([1.2840], dtype=torch.float64)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning</title>
      <link>https://YuxinZhaozyx.github.io/project/pytorch-learning/</link>
      <pubDate>Mon, 22 Jul 2019 17:37:53 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/project/pytorch-learning/</guid>
      <description>

&lt;p&gt;This project is to record my learning road of pytorch framework.&lt;/p&gt;

&lt;h2 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34; target=&#34;_blank&#34;&gt;官方教程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/zergtant/pytorch-handbook&#34; target=&#34;_blank&#34;&gt;PyTorch中文手册&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
