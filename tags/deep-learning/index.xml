<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning on YuxinZhao</title>
    <link>https://YuxinZhaozyx.github.io/tags/deep-learning/</link>
    <description>Recent content in deep learning on YuxinZhao</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year} YuxinZhao</copyright>
    <lastBuildDate>Mon, 22 Jul 2019 20:48:56 +0800</lastBuildDate>
    
	    <atom:link href="https://YuxinZhaozyx.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PyTorch Learning Note-3</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-3/</link>
      <pubDate>Mon, 22 Jul 2019 20:48:56 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-3/</guid>
      <description>

&lt;p&gt;使用torch.nn包来构建神经网络。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nn&lt;/code&gt;包依赖&lt;code&gt;autograd&lt;/code&gt;包来定义模型并求导。 一个&lt;code&gt;nn.Module&lt;/code&gt;包含各个层和一个&lt;code&gt;forward(input)&lt;/code&gt;方法，该方法返回&lt;code&gt;output&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/68747470733a2f2f7079746f7263682e6f72672f7475746f7269616c732f5f696d616765732f6d6e6973742e706e67.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。&lt;/p&gt;

&lt;p&gt;神经网络的典型训练过程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;定义包含一些可学习的参数(或者叫权重)神经网络模型；&lt;/li&gt;
&lt;li&gt;在数据集上迭代；&lt;/li&gt;
&lt;li&gt;通过神经网络处理输入；&lt;/li&gt;
&lt;li&gt;计算损失(输出结果和正确值的差值大小)；&lt;/li&gt;
&lt;li&gt;将梯度反向传播回网络的参数；&lt;/li&gt;
&lt;li&gt;更新网络的参数，主要使用如下简单的更新原则： &lt;code&gt;weight = weight - learning_rate * gradient&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;定义网络&#34;&gt;定义网络&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torch.nn as nn
import torch.nn.functional as F 

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()

        # 1 input image channel
        # 6 output image channel
        # 5x5 square convolution kernel
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)

        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # if the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)

        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        
        num_features = 1
        for s in size:
            num_features *= s
        return num_features
    
net = Net()
print(net)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;在模型中必须要定义 &lt;code&gt;forward&lt;/code&gt; 函数&lt;/strong&gt;，&lt;code&gt;backward&lt;/code&gt; 函数（用来计算梯度）会被&lt;code&gt;autograd&lt;/code&gt;自动创建。 可以在 &lt;code&gt;forward&lt;/code&gt; 函数中使用任何针对 Tensor 的操作。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;net.parameters()&lt;/code&gt;返回可被学习的参数（权重）列表和值&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;params = list(net.parameters())
print(len(params))
print(params[0].size())  # params[0] == net.conv1.weight
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;10
torch.Size([6, 1, 5, 5]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试随机输入32×32。 注：这个网络（LeNet）期望的输入大小是32×32，如果使用MNIST数据集来训练这个网络，请把图片大小重新调整到32×32。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input = torch.randn(1, 1, 32, 32)
out = net(input)
print(out)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[ 0.1052, -0.0361,  0.1122,  0.1072,  0.0887,  0.0477,
 0.0916, -0.0594,
         -0.1450,  0.0574]], grad_fn=&amp;lt;AddmmBackward&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将所有参数的梯度缓存清零，然后进行随机梯度的的反向传播：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;net.zero_grad()
out.backward(torch.randn(1, 10))
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;torch.nn&lt;/code&gt; 只支持小批量输入。整个 &lt;code&gt;torch.nn&lt;/code&gt; 包都只支持小批量样本，而不支持单个样本。 例如，&lt;code&gt;nn.Conv2d&lt;/code&gt; 接受一个4维的张量， &lt;code&gt;每一维分别是sSamples * nChannels * Height * Width（样本数*通道数*高*宽）&lt;/code&gt;。 &lt;strong&gt;如果你有单个样本，只需使用 &lt;code&gt;input.unsqueeze(0)&lt;/code&gt; 来添加其它的维数&lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;我们回顾一下到目前为止用到的类。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;回顾:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;：一个用过自动调用 &lt;code&gt;backward()&lt;/code&gt;实现支持自动梯度计算的 &lt;em&gt;多维数组&lt;/em&gt; ， 并且保存关于这个向量的梯度&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn.Module&lt;/code&gt;：神经网络模块。封装参数、移动到GPU上运行、导出、加载等。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn.Parameter&lt;/code&gt;：一种变量，当把它赋值给一个&lt;code&gt;Module&lt;/code&gt;时，被 自动地注册为一个参数。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;autograd.Function&lt;/code&gt;：实现一个自动求导操作的前向和反向定义，每个变量操作至少创建一个函数节点，每一个&lt;code&gt;Tensor&lt;/code&gt;的操作都h会创建一个接到创建&lt;code&gt;Tensor&lt;/code&gt;和 编码其历史 的函数的&lt;code&gt;Function&lt;/code&gt;节点。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;损失函数&#34;&gt;损失函数&lt;/h2&gt;

&lt;p&gt;一个损失函数接受一对 (output, target) 作为输入，计算一个值来估计网络的输出和目标值相差多少。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://pytorch.org/docs/nn&#34; target=&#34;_blank&#34;&gt;nn包&lt;/a&gt;中有很多不同的&lt;a href=&#34;https://pytorch.org/docs/nn.html#loss-functions&#34; target=&#34;_blank&#34;&gt;损失函数&lt;/a&gt;。 &lt;code&gt;nn.MSELoss&lt;/code&gt;是一个比较简单的损失函数，它计算输出和目标间的&lt;strong&gt;均方误差&lt;/strong&gt;， 例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input = torch.randn(1, 1, 32, 32)
output = net(input)
target = torch.randn(10)  # 随机值作为样例
target = target.view(1, -1)  # 使target和output的shape相同

criterion = nn.MSELoss()
loss = criterion(output, target)
print(loss)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor(1.1509, grad_fn=&amp;lt;MseLossBackward&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;反向传播&#34;&gt;反向传播&lt;/h2&gt;

&lt;p&gt;调用&lt;code&gt;loss.backward()&lt;/code&gt;获得反向传播的误差。&lt;/p&gt;

&lt;p&gt;但是在调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。&lt;/p&gt;

&lt;p&gt;现在，我们将调用&lt;code&gt;loss.backward()&lt;/code&gt;，并查看conv1层的偏差（bias）项在反向传播前后的梯度。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;net.zero_grad()  # 清除梯度

print(&#39;conv1.bias.grad before backward&#39;)
print(net.conv1.bias.grad)

loss.backward()

print(&#39;conv1.bias.grad after backward&#39;)
print(net.conv1.bias.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conv1.bias.grad before backward
None
conv1.bias.grad after backward
tensor([ 0.0027, -0.0142,  0.0197,  0.0021, -0.0018,  0.0001])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;更新权重&#34;&gt;更新权重&lt;/h2&gt;

&lt;p&gt;在实践中最简单的权重更新规则是随机梯度下降（SGD）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; weight = weight - learning_rate * gradient
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以使用简单的Python代码实现这个规则：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包&lt;code&gt;torch.optim&lt;/code&gt;实现了所有的这些规则。 使用它们非常简单：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01)
criterion = nn.MSELoss()

# in your training loop
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()   # Does the update
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning Note-2</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-2/</link>
      <pubDate>Mon, 22 Jul 2019 19:20:28 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-2/</guid>
      <description>

&lt;h1 id=&#34;autograd-自动求导机制&#34;&gt;Autograd: 自动求导机制&lt;/h1&gt;

&lt;p&gt;PyTorch 中所有神经网络的核心是 &lt;code&gt;autograd&lt;/code&gt; 包。 我们先简单介绍一下这个包，然后训练第一个简单的神经网络。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;autograd&lt;/code&gt;包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。&lt;/p&gt;

&lt;h2 id=&#34;张量-tensor&#34;&gt;张量 Tensor&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;是这个包的核心类。如果设置 &lt;code&gt;.requires_grad&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt;，那么将会追踪所有对于该张量的操作。 当完成计算后通过调用 &lt;code&gt;.backward()&lt;/code&gt;，自动计算所有的梯度， 这个张量的所有梯度将会自动积累到 &lt;code&gt;.grad&lt;/code&gt; 属性。&lt;/p&gt;

&lt;p&gt;要阻止张量跟踪历史记录，可以调用&lt;code&gt;.detach()&lt;/code&gt;方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。&lt;/p&gt;

&lt;p&gt;为了防止跟踪历史记录（和使用内存），可以将代码块包装在&lt;code&gt;with torch.no_grad()：&lt;/code&gt;中。 在评估模型时特别有用，因为模型可能具有&lt;code&gt;requires_grad = True&lt;/code&gt;的可训练参数，但是我们不需要梯度计算。&lt;/p&gt;

&lt;p&gt;在自动梯度计算中还有另外一个重要的类&lt;code&gt;Function&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Tensor&lt;/code&gt; 和 &lt;code&gt;Function&lt;/code&gt;互相连接并生成一个非循环图，它表示和存储了完整的计算历史。 每个张量都有一个&lt;code&gt;.grad_fn&lt;/code&gt;属性，这个属性引用了一个创建了&lt;code&gt;Tensor&lt;/code&gt;的&lt;code&gt;Function&lt;/code&gt;（除非这个张量是用户手动创建的，即，这个张量的 &lt;code&gt;grad_fn&lt;/code&gt; 是 &lt;code&gt;None&lt;/code&gt;）。&lt;/p&gt;

&lt;p&gt;如果需要计算导数，你可以在&lt;code&gt;Tensor&lt;/code&gt;上调用&lt;code&gt;.backward()&lt;/code&gt;。 如果&lt;code&gt;Tensor&lt;/code&gt;是一个标量（即它包含一个元素数据）则不需要为&lt;code&gt;backward()&lt;/code&gt;指定任何参数， 但是如果它有更多的元素，你需要指定一个&lt;code&gt;gradient&lt;/code&gt; 参数来匹配张量的形状。&lt;/p&gt;

&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    在其他的文章中你可能会看到说将Tensor包裹到Variable中提供自动梯度计算，Variable 这个在0.41版中已经被标注为过期了，现在可以直接使用Tensor，官方文档在&lt;a href=&#34;https://pytorch.org/docs/stable/autograd.html#variable-deprecated&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;创建一个张量并设置 &lt;code&gt;requires_grad=True&lt;/code&gt; 用来追踪他的计算历史&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.ones(2, 2, requires_grad=True)
print(x)

y = x + 2  # 对x进行操作
print(y)
print(&amp;quot;y.grad_fn: &amp;quot;, y.grad_fn)  # 结果y已经被计算出来了，所以，grad_fn已经被自动生成了

z = y * y * 3  # 对y进行操作
out = z.mean()
print(z)
print(out)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
tensor([[3., 3.],
        [3., 3.]], grad_fn=&amp;lt;AddBackward0&amp;gt;)
y.grad_fn:  &amp;lt;AddBackward0 object at 0x00000202022DAE48&amp;gt;
tensor([[27., 27.],
        [27., 27.]], grad_fn=&amp;lt;MulBackward0&amp;gt;)
tensor(27., grad_fn=&amp;lt;MeanBackward0&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;.requires_grad_( ... )&lt;/code&gt; 可以改变现有张量的 &lt;code&gt;requires_grad&lt;/code&gt;属性。 如果没有指定的话，默认输入的flag是 &lt;code&gt;False&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = torch.ones(2, 2)
a = (a * 3) / (a - 1)
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;False
True
&amp;lt;SumBackward0 object at 0x0000028141CBADA0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;梯度&#34;&gt;梯度&lt;/h2&gt;

&lt;p&gt;反向传播 因为 &lt;code&gt;out&lt;/code&gt;是一个纯量（scalar），&lt;code&gt;out.backward()&lt;/code&gt; 等于&lt;code&gt;out.backward(torch.tensor(1))&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;out.backward()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;print gradients $\frac{d(out)}{dx}$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.ones(2, 2, requires_grad=True)
y = x + 2  
z = y * y * 3  
out = z.mean()

out.backward()
print(x.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以用自动求导做更多操作&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn(3, requires_grad=True)

y = x * 2
while y.data.norm() &amp;lt; 1000:
    y = y * 2

print(y)
gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
y.backward(gradients)

print(x.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([-790.8533,  793.1236,  307.1018], grad_fn=&amp;lt;MulBackward0&amp;gt;)
tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果&lt;code&gt;.requires_grad=True&lt;/code&gt;但是你又不希望进行autograd的计算， 那么可以将变量包裹在 &lt;code&gt;with torch.no_grad()&lt;/code&gt;中:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print(x.requires_grad)
    print((x ** 2).requires_grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;True
True
True
False
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;更多阅读：&lt;/strong&gt; autograd 和 Function 的&lt;a href=&#34;https://pytorch.org/docs/autograd&#34; target=&#34;_blank&#34;&gt;官方文档&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning Note-1</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-1/</link>
      <pubDate>Mon, 22 Jul 2019 18:11:52 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-1/</guid>
      <description>

&lt;h2 id=&#34;pytorch-是什么&#34;&gt;PyTorch 是什么?&lt;/h2&gt;

&lt;p&gt;基于Python的科学计算包，服务于以下两种场景:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;作为NumPy的替代品，可以使用GPU的强大计算能力&lt;/li&gt;
&lt;li&gt;提供最大的灵活性和高速的深度学习研究平台&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tensor-张量&#34;&gt;Tensor 张量&lt;/h2&gt;

&lt;p&gt;Tensors与Numpy中的 ndarrays类似，但是在PyTorch中 Tensors 可以使用GPU进行计算.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from __future__ import print_function
import torch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建一个  5x5 的矩阵，但未初始化:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.empty(5, 3)
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[9.5511e-39, 1.0102e-38, 4.6837e-39],
        [4.9592e-39, 5.0510e-39, 9.9184e-39],
        [9.0000e-39, 1.0561e-38, 1.0653e-38],
        [4.1327e-39, 8.9082e-39, 9.8265e-39],
        [9.4592e-39, 1.0561e-38, 1.0653e-38]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建一个随机初始化的矩阵:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.rand(5, 3)
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[0.6004, 0.9095, 0.5525],
        [0.2870, 0.2680, 0.1937],
        [0.9153, 0.0150, 0.5165],
        [0.7875, 0.7397, 0.9305],
        [0.8575, 0.1453, 0.2655]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建一个0填充的矩阵，数据类型为&lt;code&gt;long&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.zeros(5, 3, dtype=torch.long)
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建tensor并使用现有数据初始化:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.tensor([5.5, 3])
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([5.5000, 3.0000])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;根据现有的张量创建张量&lt;/strong&gt;。 这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.tensor([5.5, 3])
print(x)

x = x.new_ones(5, 3, dtype=torch.double)      # new_* 方法来创建对象
print(x)

x = torch.randn_like(x, dtype=torch.float)    # 覆盖 dtype!
print(x)                                      #  对象的size 是相同的，只是值和类型发生了变化
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([5.5000, 3.0000])
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[-0.5648,  1.4639, -0.1247],
        [ 0.4187,  0.0255, -0.0938],
        [-1.2237,  0.3889,  0.9847],
        [-0.2423, -3.3706, -0.3511],
        [-1.1498, -1.1044,  0.4582]])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;获取-size&#34;&gt;获取 size&lt;/h2&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    使用&lt;code&gt;size&lt;/code&gt;方法与Numpy的&lt;code&gt;shape&lt;/code&gt;属性返回的相同，张量也支持&lt;code&gt;shape&lt;/code&gt;属性
  &lt;/div&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.ones(5, 3)
print(x)

print(&amp;quot;x.size(): &amp;quot;, x.size())
print(&amp;quot;x.shape:  &amp;quot;, x.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
x.size():  torch.Size([5, 3])
x.shape:   torch.Size([5, 3])
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;torch.Size()&lt;/code&gt;返回值是&lt;code&gt;tuple&lt;/code&gt;类型，所以它支持&lt;code&gt;tuple&lt;/code&gt;类型的所有操作
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;operation-操作&#34;&gt;Operation 操作&lt;/h2&gt;

&lt;h3 id=&#34;加法&#34;&gt;加法&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.rand(5, 3)
y = torch.rand(5, 3)

sum = x + y                 # 加法1，操作符
sum = torch.add(x, y)       # 加法2，函数
torch.add(x, y, out=sum)    # 加法3，提供输出张量sum作为参数
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;替换&#34;&gt;替换&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# add x to y
y.add_(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    任何 以&lt;code&gt;_&lt;/code&gt; 结尾的操作都会用结果替换原变量. 例如: &lt;code&gt;x.copy_(y)&lt;/code&gt;, &lt;code&gt;x.t_()&lt;/code&gt;, 都会改变 &lt;code&gt;x&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;截取&#34;&gt;截取&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x[:, 1])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;view-reshape&#34;&gt;view / reshape&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;torch.view&lt;/code&gt; 可以改变张量的维度和大小，与numpy的reshape类似&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  #  size -1 从其他维度推断
print(x.size(), y.size(), z.size())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;只有一个元素的张量取值&#34;&gt;只有一个元素的张量取值&lt;/h3&gt;

&lt;p&gt;如果你有只有一个元素的张量，使用&lt;code&gt;.item()&lt;/code&gt;来得到Python数据类型的数值&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn(1)
print(x)
print(x.item())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([-0.2036])
-0.203627809882164
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    更多操作&lt;a href=&#34;https://pytorch.org/docs/stable/torch.html&#34; target=&#34;_blank&#34;&gt;点击此处&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;numpy-转换&#34;&gt;Numpy 转换&lt;/h2&gt;

&lt;p&gt;Torch Tensor与NumPy数组&lt;strong&gt;共享底层内存地址&lt;/strong&gt;，修改一个会导致另一个的变化。&lt;/p&gt;

&lt;h3 id=&#34;torch-tensor-转换成-numpy数组&#34;&gt;Torch Tensor 转换成 NumPy数组&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = torch.ones(5)
print(a)

b = a.numpy()
print(b)

a.add_(1)
print(a)
print(b)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([1., 1., 1., 1., 1.])
[1. 1. 1. 1. 1.]
tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;numpy数组-转换成-torch-tensor&#34;&gt;NumPy数组 转换成 Torch Tensor&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = np.ones(5)
b = torch.from_numpy(a)
print(a)
print(b)

np.add(a, 1, out=a)
print(a)
print(b)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[1. 1. 1. 1. 1.]
tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    所有的 &lt;code&gt;Tensor&lt;/code&gt; 类型默认都是基于CPU， &lt;code&gt;CharTensor&lt;/code&gt; 类型不支持到 NumPy 的转换.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;cuda-张量&#34;&gt;CUDA 张量&lt;/h2&gt;

&lt;p&gt;使用&lt;code&gt;.to&lt;/code&gt; 方法 可以将Tensor移动到任何设备中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.rand(1)

# is_available 函数判断是否有cuda可以使用
# torch.device 将张量移动到指定的设备中
if torch.cuda.is_available():
    device = torch.device(&amp;quot;cuda&amp;quot;)          # a CUDA 设备对象
    y = torch.ones_like(x, device=device)  # 直接从GPU创建张量
    x = x.to(device)                       # 或者直接使用 .to(&amp;quot;cuda&amp;quot;) 将张量移动到cuda中
    z = x + y
    print(z)
    print(z.to(&amp;quot;cpu&amp;quot;, torch.double))       # .to 也会对变量的类型做更改
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([1.2840], device=&#39;cuda:0&#39;)
tensor([1.2840], dtype=torch.float64)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning</title>
      <link>https://YuxinZhaozyx.github.io/project/pytorch-learning/</link>
      <pubDate>Mon, 22 Jul 2019 17:37:53 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/project/pytorch-learning/</guid>
      <description>

&lt;p&gt;This project is to record my learning road of pytorch framework.&lt;/p&gt;

&lt;h2 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34; target=&#34;_blank&#34;&gt;官方教程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/zergtant/pytorch-handbook&#34; target=&#34;_blank&#34;&gt;PyTorch中文手册&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>[论文笔记] R-C3D: Region Convolutional 3D Network for Temporal Activity Detection</title>
      <link>https://YuxinZhaozyx.github.io/paper-note/r-c3d-region-convolutional-3d-network-for-temporal-activity-detection/</link>
      <pubDate>Wed, 17 Jul 2019 18:44:03 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/paper-note/r-c3d-region-convolutional-3d-network-for-temporal-activity-detection/</guid>
      <description>

&lt;p&gt;作者提出了R-C3D模型用于连续视频的行为检测(Activity Detection in Continuous Videos)。连续视频的行为检测需要完成两个目标：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;识别出行为的类别&lt;/li&gt;
&lt;li&gt;定位行为发生的时间范围&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这两个问题正是R-C3D着力解决的。&lt;/p&gt;

&lt;h3 id=&#34;术语缩写&#34;&gt;术语缩写&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;缩写&lt;/th&gt;
&lt;th&gt;全称&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;R-C3D&lt;/td&gt;
&lt;td&gt;Region Convolutional 3D Network&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;RoI&lt;/td&gt;
&lt;td&gt;Region of Interest&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;RPN&lt;/td&gt;
&lt;td&gt;Region Proposal Network&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;r-c3d-的特点&#34;&gt;R-C3D 的特点&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;端到端的训练方式；&lt;/li&gt;
&lt;li&gt;可以检测出任意时长的行为；&lt;/li&gt;
&lt;li&gt;检测速度快，一次性能计算的帧仅受限于GPU内存；&lt;/li&gt;
&lt;li&gt;推广Faster-RCNN的Region Proposal Network到时域；&lt;/li&gt;
&lt;li&gt;推广Faster-RCNN的RoI Pooling算法到时域，提出3D RoI Pooling。&lt;/li&gt;
&lt;li&gt;有监督学习(特别之处是允许一个视频中包含多种行为，且行为的时间范围有重叠)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;r-c3d-的网络结构&#34;&gt;R-C3D 的网络结构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;image/1563363180600.png&#34; alt=&#34;1563363180600&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/R-C3D.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    上图为本人理解后绘制的网络结构图，如有错误，欢迎批评指正。
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;作者提出的 R-C3D 网络包含3个组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;a shared 3D ConvNet feature extractor&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;a temporal proposal stage&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;an activity classification and refinement stage&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;3d-convolutional-feature-hierarchies&#34;&gt;3D Convolutional Feature Hierarchies&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt; sequence of RGB video frames with dimension $\mathbb{R}^{3 \times L \times H \times W}$.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The input to the model is of &lt;strong&gt;variable length&lt;/strong&gt; (&lt;strong&gt;$L$ can be arbitrary&lt;/strong&gt; and is only limited by memory)&lt;/li&gt;
&lt;li&gt;Adopt &lt;strong&gt;the convolutional layers (conv1a to conv5b) of C3D&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;本例中，$H=W=112$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt; a feature map $C_{conv5b} \in \mathbb{R}^{512 \times \frac{L}8 \times \frac{H}{16} \times \frac{W}{16} }$ (512 is the channel dimension of the layer conv5b) (activations)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;$C_{conv5b}$ activations are the shared input to the proposal and classification subnets&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;temporal-proposal-subnet&#34;&gt;Temporal Proposal Subnet&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;image/1563368283613.png&#34; alt=&#34;1563368283613&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Function:&lt;/strong&gt; predicts &lt;strong&gt;potential proposal segments with respect to anchor segments&lt;/strong&gt; and &lt;strong&gt;a binary label indicating whether the predicted proposal contains an activity or not&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The anchor segments are pre-defined multi-scale windows centered at $L/8$ uniformly distributed temporal locations.&lt;/li&gt;
&lt;li&gt;Each temporal locaiton specifies $K$ anchor segments, each at a different fixed scale. Thus, the total number of anchor segments is $(L/8) * K$ .&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;3D convolutional filter with kernel size $3 \times 3 \times 3$&lt;/strong&gt; on top of $C_{conv5b}$ to extend the temporal proposal subnet.&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;3D max-pooling filter with kernel size $1 \times \frac{H}{16} \times \frac{W}{16}$&lt;/strong&gt; to downsample the spatial dimensions (from $\frac{H}{16} \times \frac{W}{16}$ to $1 \times 1$) to a temporal only feature map $C_{tpn} \in \mathbb{R}^{512 \times \frac{L}8 \times 1 \times 1}$.&lt;/li&gt;
&lt;li&gt;The 512-dimensional feature vector at each temporal location ($512 \times 1 \times 1 \times 1$) in $C_{tpn}$ is used &lt;strong&gt;to predict&lt;/strong&gt; :

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;a relative offset $\{ \delta c_i, \delta l_i \}$ to the center location&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the length of each anchor segment $\{ c_i, l_i \},\, i \in \{ 1, \cdots, K \}$&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the binary scores for each proposal being an activity or background&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;two $1 \times 1 \times 1$ convolutional layers&lt;/strong&gt; on top of $C_{tpn}$ to predict proposal offsets and scores.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;training-temporal-proposal-subnet&#34;&gt;Training Temporal Proposal Subnet&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Positive Label:&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;if the anchor segment &lt;strong&gt;overlaps with some ground-truth activity with IoU &amp;gt; 0.7&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;or if the anchor segment has the &lt;strong&gt;highest IoU overlap with some ground-truth activity&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Negative Label:&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;if the anchor has &lt;strong&gt;IoU overlap lower than 0.3 with all ground-truth activities&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;All others are held out from training&lt;/li&gt;
&lt;li&gt;sample balanced batches with a &lt;strong&gt;positive/negative ratio of 1:1&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;activity-classification-subnet&#34;&gt;Activity Classification Subnet&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;image/1563371361436.png&#34; alt=&#34;1563371361436&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Functions:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;selecting proposal segments&lt;/strong&gt; from the previous stage.&lt;/li&gt;
&lt;li&gt;three-dimensional region of interest &lt;strong&gt;(3D RoI) pooling to extract fixed-size features for selected proposals&lt;/strong&gt;.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;activity classification and boundary regression&lt;/strong&gt; for the selected proposals based on the pooled features.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;greedy &lt;strong&gt;Non-Maximum Suppression (NMS, threshold=0.7)&lt;/strong&gt; to eliminate highly overlapping and low confidence proposals.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;3d-roi-pooling&#34;&gt;3D RoI Pooling&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;对于一个 $l \times h \times w$ 的不定输入张量（每次输入的张量的尺寸可以不一样），3D RoI Pooling 将其规约到固定的大小 $l_s \times h_s \times w_s$ .

&lt;ul&gt;
&lt;li&gt;$l \times h \times w$ 先被分成 $l_s \times h_s \times w_s$ 个大小约为 $\frac{l}{l_s} \times \frac{h}{h_s} \times \frac{w}{w_s}$ 的小张量，每个张量内做max pooling 得到 $l_s \times h_s \times w_s$ 的张量。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;在本例中，由于$H=W=112$，$C_{conv5b} \in \mathbb{R}^{512 \times \frac{L}8 \times 7 \times 7}$，将 $512 \times \frac{L}8 \times 7 \times 7$  的张量固定为 $512 \times 1 \times 4 \times 4$.&lt;/li&gt;
&lt;li&gt;The output of the 3D RoI pooling is fed to a series of &lt;strong&gt;two fully connected layers&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;3D RoI pooling 的两层 fully connected layers 之后是 classification layer 和 regression layer.

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;classification layer 和 regression layer 是两个独立的双层 fully connected layers, 它们的输入都是 3D RoI pooling 后 fully connected layers 的输出。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;training-classification-subnet&#34;&gt;Training Classification Subnet&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Positive Label:&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;if the proposal has the &lt;strong&gt;highest IoU&lt;/strong&gt; overlap with a ground-truth activity and &lt;strong&gt;IoU &amp;gt; 0.5&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Negative Label:&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;proposals with IoU overlap lower than 0.5 with all grouth-truth activities.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Positive : Negative = 1 : 3&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;optimization&#34;&gt;Optimization&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Loss Function:&lt;/strong&gt;
$$
Loss = \frac1{N_{cls}} \sum_i L_{cls}(a_i, a_i^*) + \lambda \frac1{N_{reg}} a_i^* L_{reg}(t_i, t_i^*)
$$&lt;/p&gt;

&lt;p&gt;where,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$L_{cls}$ is softmax loss function&lt;/li&gt;
&lt;li&gt;$L_{reg}$ is smooth L1 loss function&lt;/li&gt;
&lt;li&gt;$N_{cls}$ is batch size&lt;/li&gt;
&lt;li&gt;$N_{reg}$ is the number of anchor/proposal segments&lt;/li&gt;
&lt;li&gt;$\lambda$ is the loss trade-off parameter and is set to 1.&lt;/li&gt;
&lt;li&gt;$i$ is the anchor/proposal segments index in a batch&lt;/li&gt;
&lt;li&gt;$a_i$ is the predicted probability of the proposal or activities&lt;/li&gt;
&lt;li&gt;$a_i^*$ is the ground truth&lt;/li&gt;
&lt;li&gt;$t_i = \{ \delta \hat{ c_i} , \delta \hat{ l_i} \}$ represents predicted relative offset to anchor segments or proposals.&lt;/li&gt;
&lt;li&gt;$t_i^* = \{ \delta c_i , \delta l_i \}$ represents the coordinate transformation of ground truth segments to anchor segments or proposals.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
\begin{cases}
\delta c_i = (c_i^* - c_i) / l_i \\&lt;br /&gt;
\delta l_i = \log (l_i^* / l_i)
\end{cases}
$$&lt;/p&gt;

&lt;p&gt;where $c_i$ and $l_i$ are the center location and the length of anchor segments or proposals while $c_i^*$ and $l_i^*$  denote the same for the ground truth activity segment.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The above loss function is applied for both the temporal proposal subnet and the activity classification subnet.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In proposal subnet,&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;$L_{cls}$ predicts whether the proposal contains an activity or not.&lt;/li&gt;
&lt;li&gt;$L_{reg}$ optimizes the relative displacement between proposals and ground truths.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In classification subnet,&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;$L_{cls}$ predicts the specific activity class for the proposal. (The number of classes are the number of activities + one for background)&lt;/li&gt;
&lt;li&gt;$L_{reg}$ optimizes the relative displacement between activities and ground truths.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;prediction&#34;&gt;Prediction&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;预测出$t_i$后需要逆变换把相对坐标变成绝对坐标。&lt;/li&gt;
&lt;li&gt;为了充分利用向量化实现的优势，用最后一帧去填充视频较短不足的部分。&lt;/li&gt;
&lt;li&gt;NMS at a lower threshold (0.1 less than the mAP evalution threshold) is appled to the predicted activities to get the final activity predictions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&#34;experiments-on-thumos-14&#34;&gt;Experiments on THUMOS&amp;rsquo;14&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;divide 200 untrimmed videos from the validation set into 180 training and 20 held out videos to get the best hyperparameter setting.&lt;/li&gt;
&lt;li&gt;Since the GPU memory is limited, the authors first create a buffer of 768 frames at 25 fps which means approximately 30 seconds of video.&lt;/li&gt;
&lt;li&gt;The authors create the buffer by sliding from the beginning of the video to the end, denoted as the &amp;ldquo;one-way buffer&amp;rdquo;. An additional pass from the end of the video to the beginning is used to increase the amount of training data, denoted as &amp;ldquo;two-way buffer&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;initialize the 3D ConvNet part of our model with C3D weights trained on Sports-1M and finetuned on UCF101.&lt;/li&gt;
&lt;li&gt;allow all the layers of R-C3D to be trained on THUMOS&amp;rsquo;14 with a f&lt;strong&gt;ixed learning rate of 0.0001&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;K = 10, with scale values [2, 4, 5, 6, 8, 9, 10, 12, 14, 16]&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;experiments-on-activitynet&#34;&gt;Experiments on ActivityNet&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;sample frames at 3 fps&lt;/li&gt;
&lt;li&gt;input buffer: 768&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;K=20, with scale values [1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 16, 20, 24, 28, 32, 40, 48, 56, 64]&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;experiments-on-charades&#34;&gt;Experiments on Charades&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;sample frames at 5 fps&lt;/li&gt;
&lt;li&gt;input buffer: 768&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;K=18, with scale values [1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 16, 20, 24, 28, 32, 40, 48]&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>[论文笔记] Faster R-CNN: Towards Real-Time Object Detection With Region Proposal Networks</title>
      <link>https://YuxinZhaozyx.github.io/paper-note/faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks/</link>
      <pubDate>Sun, 14 Jul 2019 13:40:12 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/paper-note/faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks/</guid>
      <description>

&lt;p&gt;Faster R-CNN在Fast R-CNN的基础上做改进，提出用RPN（Region Proposal Network, 一种全卷积神经网络）代替Selective Search，降低检测耗时。Faster R-CNN由RPN和Fast R-CNN构成，RPN和Fast R-CNN共享卷积计算得到的特征图，以此降低计算量，使得Faster R-CNN可以在单GPU上以5fps的速度运行，且精度达到SOTA。&lt;/p&gt;

&lt;h3 id=&#34;术语缩写&#34;&gt;术语缩写&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;缩写&lt;/th&gt;
&lt;th&gt;全称&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;R-CNN&lt;/td&gt;
&lt;td&gt;Region Convolution Neural Network&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;RPN&lt;/td&gt;
&lt;td&gt;Region Proposal Network&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;FCN&lt;/td&gt;
&lt;td&gt;Fully Convolutional Network&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;SS&lt;/td&gt;
&lt;td&gt;Selective Search&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;ZF&lt;/td&gt;
&lt;td&gt;Zeiler and Fergus model&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;IoU&lt;/td&gt;
&lt;td&gt;Intersection-over-Union&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;RoI&lt;/td&gt;
&lt;td&gt;Region of Interest&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;faster-r-cnn&#34;&gt;Faster R-CNN&lt;/h2&gt;







&lt;figure&gt;

&lt;img src=&#34;image/1563085747876.png&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Faster R-CNN is a single, unified network for object detection. The RPN module serves as the &amp;lsquo;attention&amp;rsquo; of the unified network.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Faster R-CNN 由以下两个部分组成:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Deep fully convolutional network that proposes regions&lt;/li&gt;
&lt;li&gt;Fast R-CNN detector that uses the proposed regions&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The RPN module tells the Fast R-CNN module where to look.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;region-proposal-network&#34;&gt;Region Proposal Network&lt;/h2&gt;







&lt;figure&gt;

&lt;img src=&#34;image/1563086507733.png&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Region Proposal Network&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.&lt;/p&gt;

&lt;p&gt;The authors model this process with a &lt;strong&gt;fully convolutional network&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To generate region proposals, the authors &lt;strong&gt;slide a small network($n \times n$ spatial window, $n=3$) over the convolutional feature map&lt;/strong&gt; output by the last shared convolution layer.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Each sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU following).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This feature is &lt;strong&gt;fed into two sibling fully-connected layers&lt;/strong&gt; &amp;ndash; a &lt;strong&gt;box-regression layer&lt;/strong&gt; ($reg$) and a &lt;strong&gt;box-classification layer&lt;/strong&gt; ($cls$). This architecture is naturally implemented with and $n \times n$ convolutional layer followed by two sibling $1 \times 1$ convolutional layers (for $reg$ and $cls$ respectively).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;anchor&#34;&gt;Anchor&lt;/h3&gt;

&lt;p&gt;the number of maximum possible proposals for each location is denoted as $k$.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$reg$ layer has $4k$ outputs $(x, y, w, h)$ encoding the coordinates of $k$ boxes&lt;/li&gt;
&lt;li&gt;$cls$ layer has $2k$ outputs scores that estimate probability of object or not object for each proposal. (implemented as a two-class softmax layer)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The $k$ proposals are parameterized relative to $k$ reference boxes, which we call &lt;strong&gt;anchors&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;An anchor is centered at the sliding window in question, and is associated  with &lt;strong&gt;a scale and aspect ratio&lt;/strong&gt; (宽高比).&lt;/li&gt;
&lt;li&gt;For a convolutional feature map of a size $W \times H $ , there are $WHk$ anchors in total.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;use 3 scales with box areas of $128^2$, $256^2$, $512^2$ pixels, and 3 aspect ratios of 1:1, 1:2, 2:1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/1563093098483.png&#34; alt=&#34;1563093098483&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;multi-scale-anchors-as-regression-reference&#34;&gt;Multi-Scale Anchors as Regression Reference&lt;/h4&gt;







&lt;figure&gt;

&lt;img src=&#34;image/1563088410857.png&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Different schemes for addressing multiple scales and sizes.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;作者采取b)和c)变化窗口形状和大小的方法，而不使用a)变换图片大小的方法。&lt;/p&gt;

&lt;h3 id=&#34;lost-function&#34;&gt;Lost Function&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Positive anchor&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the anchor/anchors with the highest Intersection-over-Union(IoU) overlap with a ground-truth box (作者采用).&lt;/li&gt;
&lt;li&gt;an anchor that has an IoU overlap &amp;gt; 0.7 with any grouth-truth box (作者不采用).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Negative anchor&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;non-positive anchor if its IoU &amp;lt; 0.3 for all grouth-truth boxes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;lost-function-for-an-image&#34;&gt;lost  function for an image&lt;/h4&gt;

&lt;p&gt;$$
L(\{p_i\}, \{t_i\}) = \frac1{N_{cls}} \sum_i{L_{cls}} (p_i, p_i^*) + \lambda \frac{1}{N_{reg}} \sum_i {L_{reg}(t_i, t_i^*)}
$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$L_{cls}$ is log loss over two classes (object $vs.$ not object)&lt;/li&gt;
&lt;li&gt;$L_{reg}(t_i, t_i^*) = R(t_i - t_i^*)$ where $R$ is the robust loss function (smooth $L_1$)&lt;/li&gt;
&lt;li&gt;$N_{cls}$ is the mini-batch size (i.e., $N_{cls}=256$)&lt;/li&gt;
&lt;li&gt;$N_{reg}$ is the number of anchor locations (i.e., $N_{reg} \approx 2400$)&lt;/li&gt;
&lt;li&gt;$\lambda = 10$ and thus both $cls$ and $reg$ terms are roughly equally weighted.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The normalization as above is not required and could be simplified.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;$$
\begin{align}
&amp;amp;t_x = (x-x_a)/w_a, &amp;amp; t_y = (y-y_a)/h_a, \\&lt;br /&gt;
&amp;amp;t_w = \log(w/w_a), &amp;amp; t_h = log(h/h_a),  \\&lt;br /&gt;
&amp;amp;t^*_x = (x-x^*_a)/w_a, &amp;amp; t^*_y = (y^*-y_a)/h_a, \\&lt;br /&gt;
&amp;amp;t^*_w = \log(w^*/w_a), &amp;amp; t^*_h = log(h^*/h_a),  \\&lt;br /&gt;
\end{align}
$$&lt;/p&gt;

&lt;h4 id=&#34;training-rpns&#34;&gt;Training RPNs&lt;/h4&gt;

&lt;p&gt;The RPN can be trained &lt;strong&gt;end-to-end&lt;/strong&gt; by back-propagation and &lt;strong&gt;stochastic gradient descent (SGD)&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一张图片包含多个正样本和负样本(正样本少于负样本)&lt;/li&gt;
&lt;li&gt;随机采样256个样本用于计算loss of a mini-batch&lt;/li&gt;
&lt;li&gt;初始化: Gaussian distribution (mean=0, standard deviation=0.01)&lt;/li&gt;
&lt;li&gt;momentum: 0.9,   weight decay: 0.0005&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;sharing-features-for-rpn-and-fast-r-cnn&#34;&gt;Sharing Features for RPN and Fast R-CNN&lt;/h2&gt;

&lt;h3 id=&#34;4-step-alternating-training&#34;&gt;4 - Step Alternating Training&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Train the RPN as described previously.&lt;/li&gt;
&lt;li&gt;Train a separated detection network (ImageNet-pre-trained) by Fast R-CNN using the proposals generated by step-1 RPN.&lt;/li&gt;
&lt;li&gt;Use the detector network to initialize RPN training, but fix the shared convolutional layers and only fine-tune the layers unique to RPN.&lt;/li&gt;
&lt;li&gt;keeping the shared convolutional layers fixed, fine-tune the unique layers of Fast R-CNN.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
