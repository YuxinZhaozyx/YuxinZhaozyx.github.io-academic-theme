<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pytorch on YuxinZhao</title>
    <link>https://YuxinZhaozyx.github.io/tags/pytorch/</link>
    <description>Recent content in pytorch on YuxinZhao</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year} YuxinZhao</copyright>
    <lastBuildDate>Tue, 23 Jul 2019 11:10:54 +0800</lastBuildDate>
    
	    <atom:link href="https://YuxinZhaozyx.github.io/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PyTorch Learning Note-5</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-5/</link>
      <pubDate>Tue, 23 Jul 2019 11:10:54 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-5/</guid>
      <description>

&lt;p&gt;本文将学习如何使用 &lt;code&gt;DataParallel&lt;/code&gt; 来使用多GPU。&lt;/p&gt;

&lt;p&gt;PyTorch非常容易就可以使用多GPU，用如下方式把一个模型放到GPU上：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;device = torch.device(&amp;quot;cuda:0&amp;quot;)
model.to(device)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后复制所有的张量到GPU上：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mytensor = my_tensor.to(device)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    只调用&lt;code&gt;my_tensor.to(device)&lt;/code&gt;并没有复制张量到GPU上，而是返回了一个copy。所以你需要把它赋值给一个新的张量并在GPU上使用这个张量。
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;在多GPU上执行前向和反向传播是自然而然的事。 但是&lt;strong&gt;PyTorch默认将只使用一个GPU&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;使用&lt;code&gt;DataParallel&lt;/code&gt;可以轻易的让模型并行运行在多个GPU上。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = nn.DataParallel(model)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;导入模块和定义参数&#34;&gt;导入模块和定义参数&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Parameters and DataLoaders
input_size = 5
output_size = 2

batch_size = 30
data_size = 100

# Device
device = torch.device(&amp;quot;cuda:0&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;虚拟数据集&#34;&gt;虚拟数据集&lt;/h2&gt;

&lt;p&gt;制作一个虚拟（随机）数据集， 你只需实现 &lt;code&gt;__getitem__&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class RandomDataset(Dataset):

    def __init__(self, size, length):
        self.len = length
        self.data = torch.randn(length, size)

    def __getitem__(self, index):
        return self.data[index]

    def __len__(self):
        return self.len


rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;简单模型&#34;&gt;简单模型&lt;/h2&gt;

&lt;p&gt;作为演示，我们的模型只接受一个输入，执行一个线性操作，然后得到结果。 说明：&lt;code&gt;DataParallel&lt;/code&gt;能在任何模型（CNN，RNN，Capsule Net等）上使用。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Model(nn.Module):
    
    def __init__(self, input_size, output_size):
        super(Model, self).__init__()
        self.fc = nn.Linear(input_size, output_size)

    def forward(self, input):
        output = self.fc(input)
        print(&amp;quot;\t In Model: input size&amp;quot;, input.size(), &amp;quot;output size&amp;quot;, output.size())
        return output
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;创建一个模型和数据并行&#34;&gt;创建一个模型和数据并行&lt;/h2&gt;

&lt;p&gt;首先，我们需要创建一个模型实例和检测我们是否有多个GPU。 如果有多个GPU，使用&lt;code&gt;nn.DataParallel&lt;/code&gt;来包装我们的模型。 然后通过&lt;code&gt;model.to(device)&lt;/code&gt;把模型放到GPU上。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = Model(input_size, output_size)
if torch.cuda.device_count() &amp;gt; 1:
    print(&amp;quot;Let&#39;s use&amp;quot;, torch.cuda.device_count(), &amp;quot;GPUs!&amp;quot;)
    # dim = 0 [30, xxx] -&amp;gt; [10, ...], [10, ...], [10, ...] on 3 GPUs
    model = nn.DataParallel(model)

model.to(device)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;运行模型&#34;&gt;运行模型&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for data in rand_loader:
    input = data.to(device)
    output = model(input)
    print(&amp;quot;Outside: input size&amp;quot;, input.size(), &amp;quot;output_size&amp;quot;, output.size())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当没有或者只有一个GPU时，对30个输入和输出进行批处理，得到了期望的一样得到30个输入和输出，但是如果你有多个GPU，你得到如下的结果。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2 GPUs&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Let&#39;s use 2 GPUs!
    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])
    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])
Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning Note-4</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-4/</link>
      <pubDate>Tue, 23 Jul 2019 09:40:02 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-4/</guid>
      <description>

&lt;p&gt;一般情况下处理图像、文本、音频和视频数据时，可以使用标准的Python包来加载数据到一个numpy数组中。 然后把这个数组转换成 &lt;code&gt;torch.*Tensor&lt;/code&gt;。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;图像可以使用 Pillow, OpenCV&lt;/li&gt;
&lt;li&gt;音频可以使用 scipy, librosa&lt;/li&gt;
&lt;li&gt;文本可以使用原始Python和Cython来加载，或者使用 NLTK或 SpaCy 处理&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;特别的，对于图像任务，&lt;code&gt;torchvision&lt;/code&gt; 包 包含了处理一些基本图像数据集的方法。这些数据集包括 Imagenet, CIFAR10, MNIST 等。除了数据加载以外，&lt;code&gt;torchvision&lt;/code&gt; 还包含了图像转换器， &lt;code&gt;torchvision.datasets&lt;/code&gt; 和 &lt;code&gt;torch.utils.data.DataLoader&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;torchvision&lt;/code&gt;包不仅提供了巨大的便利，也避免了代码的重复。&lt;/p&gt;

&lt;p&gt;以下使用的案例中，将使用CIFAR10数据集，它有如下10个类别 ：‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。CIFAR-10的图像都是 3x32x32大小的，即，3颜色通道，32x32像素。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/4ed11e8caab93da7138bafcf8d14441abe097151/68747470733a2f2f7079746f7263682e6f72672f7475746f7269616c732f5f696d616765732f636966617231302e706e67&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;训练一个图像分类器&#34;&gt;训练一个图像分类器&lt;/h1&gt;

&lt;p&gt;依次按照下列顺序进行：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;使用&lt;code&gt;torchvision&lt;/code&gt;加载和归一化CIFAR10训练集和测试集&lt;/li&gt;
&lt;li&gt;定义一个卷积神经网络&lt;/li&gt;
&lt;li&gt;定义损失函数&lt;/li&gt;
&lt;li&gt;在训练集上训练网络&lt;/li&gt;
&lt;li&gt;在测试集上测试网络&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;读取和归一化-cifar10&#34;&gt;读取和归一化 CIFAR10&lt;/h2&gt;

&lt;p&gt;使用&lt;code&gt;torchvision&lt;/code&gt;可以非常容易地加载CIFAR10。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torchvision
import torchvision.transforms as transforms 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;torchvision的输出是[0,1]的PILImage图像，我们把它转换为归一化范围为[-1, 1]的张量。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)

classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;展示一些训练图像&#34;&gt;展示一些训练图像&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt 
import numpy as np 

# 展示图像的函数
def imshow(img):
    img = img / 2 + 0.5  # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# 获取随机数据
dataiter = iter(trainloader)
images, labels = dataiter.next()

# 展示图像
imshow(torchvision.utils.make_grid(images))

# 显示图像标签
print(&amp;quot; &amp;quot;.join(&amp;quot;%5s&amp;quot; % classes[labels[j]] for j in range(4)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;image/1563855333962.png&#34; alt=&#34;1563855333962&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;truck horse   dog  frog
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;定义一个卷积神经网络&#34;&gt;定义一个卷积神经网络&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch.nn as nn
import torch.nn.functional as F 

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        
        self.conv1 = nn.Conv2d(3, 6, 5) # 3 input channels, 6 output channels, 5x5 convolutional kernel
        self.pool = nn.MaxPool2d(2, 2)  # 池化层可以共用
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
    
net = Net()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;定义损失函数和优化器&#34;&gt;定义损失函数和优化器&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;训练网络&#34;&gt;训练网络&lt;/h2&gt;

&lt;p&gt;我们只需在数据迭代器上循环，将数据输入给网络，并优化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for epoch in range(2):  # 多批次循环

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # 获取输入
        inputs, labels = data

        # 梯度置0
        optimizer.zero_grad()

        # 正向传播，反向传播，优化
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # 打印状态信息
        running_loss += loss.item()
        if i % 2000 == 1999: # 每2000批次打印一次
            print(&#39;[%d, %5d] loss: %.3f&#39; % (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print(&amp;quot;Finished Training&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;在测试集上测试网络&#34;&gt;在测试集上测试网络&lt;/h2&gt;

&lt;p&gt;我们在整个训练集上进行了2次训练，但是我们需要检查网络是否从数据集中学习到有用的东西。 通过预测神经网络输出的类别标签与实际情况标签进行对比来进行检测。 如果预测正确，我们把该样本添加到正确预测列表。 第一步，显示测试集中的图片并熟悉图片内容。然后用&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataiter = iter(testloader)
images, labels = dataiter.next()

# 输出是10个标签的能量。 一个类别的能量越大，神经网络越认为它是这个类别。所以让我们得到最高能量的标签。
outputs = net(images) 
_, predicted = torch.max(outputs, 1) # (maxvalues, indices) is returned

# 显示图片
imshow(torchvision.utils.make_grid(images))
print(&#39;GroundTruth: &#39;, &#39; &#39;.join(&#39;%5s&#39; % classes[labels[j]] for j in range(4)))
print(&#39;Predicted:   &#39;, &#39; &#39;.join(&#39;%5s&#39; % classes[predicted[j]] for j in range(4)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;image/1563857966974.png&#34; alt=&#34;1563857966974&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;GroundTruth:    cat  ship  ship plane
Predicted:     bird plane plane   dog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来让看看网络在整个测试集上的结果如何。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(&#39;Accuracy of the network on the 10000 test images: %d %%&#39; % (100 * correct / total))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Accuracy of the network on the 10000 test images: 49 %
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在识别哪一类的时候好，哪一类不好呢？&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs, 1)
        c = (predicted == labels).squeeze()
        for i in range(4):
            label = labels[i]
            class_correct[label] += c[i].item()
            class_total[label] += 1

for i in range(10):
    print(&#39;Accuracy of %5s : %2d %%&#39; % (classes[i], 100 * class_correct[i] / class_total[i]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Accuracy of plane : 69 %
Accuracy of   car : 69 %
Accuracy of  bird : 15 %
Accuracy of   cat : 43 %
Accuracy of  deer : 44 %
Accuracy of   dog : 22 %
Accuracy of  frog : 72 %
Accuracy of horse : 58 %
Accuracy of  ship : 41 %
Accuracy of truck : 58 %
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;在gpu上训练&#34;&gt;在GPU上训练&lt;/h2&gt;

&lt;p&gt;把一个神经网络移动到GPU上训练就像把一个Tensor转换GPU上一样简单。并且这个操作会递归遍历有所模块，并将其参数和缓冲区转换为CUDA张量。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;device = torch.device(&amp;quot;cuda:0&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)

# 确认我们的电脑支持CUDA，然后显示CUDA信息
print(device)

# 将递归遍历所有模块并将模块的参数和缓冲区 转换成CUDA张量
net.to(device)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;记住: &lt;code&gt;inputs&lt;/code&gt; 和 &lt;code&gt;labels&lt;/code&gt; 也要转换。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;inputs, labels = inputs.to(device), labels.to(device)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning Note-3</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-3/</link>
      <pubDate>Mon, 22 Jul 2019 20:48:56 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-3/</guid>
      <description>

&lt;p&gt;使用torch.nn包来构建神经网络。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nn&lt;/code&gt;包依赖&lt;code&gt;autograd&lt;/code&gt;包来定义模型并求导。 一个&lt;code&gt;nn.Module&lt;/code&gt;包含各个层和一个&lt;code&gt;forward(input)&lt;/code&gt;方法，该方法返回&lt;code&gt;output&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image/68747470733a2f2f7079746f7263682e6f72672f7475746f7269616c732f5f696d616765732f6d6e6973742e706e67.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。&lt;/p&gt;

&lt;p&gt;神经网络的典型训练过程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;定义包含一些可学习的参数(或者叫权重)神经网络模型；&lt;/li&gt;
&lt;li&gt;在数据集上迭代；&lt;/li&gt;
&lt;li&gt;通过神经网络处理输入；&lt;/li&gt;
&lt;li&gt;计算损失(输出结果和正确值的差值大小)；&lt;/li&gt;
&lt;li&gt;将梯度反向传播回网络的参数；&lt;/li&gt;
&lt;li&gt;更新网络的参数，主要使用如下简单的更新原则： &lt;code&gt;weight = weight - learning_rate * gradient&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;定义网络&#34;&gt;定义网络&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torch.nn as nn
import torch.nn.functional as F 

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()

        # 1 input image channel
        # 6 output image channel
        # 5x5 square convolution kernel
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)

        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # if the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)

        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        
        num_features = 1
        for s in size:
            num_features *= s
        return num_features
    
net = Net()
print(net)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;在模型中必须要定义 &lt;code&gt;forward&lt;/code&gt; 函数&lt;/strong&gt;，&lt;code&gt;backward&lt;/code&gt; 函数（用来计算梯度）会被&lt;code&gt;autograd&lt;/code&gt;自动创建。 可以在 &lt;code&gt;forward&lt;/code&gt; 函数中使用任何针对 Tensor 的操作。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;net.parameters()&lt;/code&gt;返回可被学习的参数（权重）列表和值&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;params = list(net.parameters())
print(len(params))
print(params[0].size())  # params[0] == net.conv1.weight
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;10
torch.Size([6, 1, 5, 5]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试随机输入32×32。 注：这个网络（LeNet）期望的输入大小是32×32，如果使用MNIST数据集来训练这个网络，请把图片大小重新调整到32×32。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input = torch.randn(1, 1, 32, 32)
out = net(input)
print(out)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[ 0.1052, -0.0361,  0.1122,  0.1072,  0.0887,  0.0477,
 0.0916, -0.0594,
         -0.1450,  0.0574]], grad_fn=&amp;lt;AddmmBackward&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将所有参数的梯度缓存清零，然后进行随机梯度的的反向传播：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;net.zero_grad()
out.backward(torch.randn(1, 10))
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;torch.nn&lt;/code&gt; 只支持小批量输入。整个 &lt;code&gt;torch.nn&lt;/code&gt; 包都只支持小批量样本，而不支持单个样本。 例如，&lt;code&gt;nn.Conv2d&lt;/code&gt; 接受一个4维的张量， &lt;code&gt;每一维分别是sSamples * nChannels * Height * Width（样本数*通道数*高*宽）&lt;/code&gt;。 &lt;strong&gt;如果你有单个样本，只需使用 &lt;code&gt;input.unsqueeze(0)&lt;/code&gt; 来添加其它的维数&lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;我们回顾一下到目前为止用到的类。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;回顾:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;：一个用过自动调用 &lt;code&gt;backward()&lt;/code&gt;实现支持自动梯度计算的 &lt;em&gt;多维数组&lt;/em&gt; ， 并且保存关于这个向量的梯度&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn.Module&lt;/code&gt;：神经网络模块。封装参数、移动到GPU上运行、导出、加载等。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn.Parameter&lt;/code&gt;：一种变量，当把它赋值给一个&lt;code&gt;Module&lt;/code&gt;时，被 自动地注册为一个参数。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;autograd.Function&lt;/code&gt;：实现一个自动求导操作的前向和反向定义，每个变量操作至少创建一个函数节点，每一个&lt;code&gt;Tensor&lt;/code&gt;的操作都h会创建一个接到创建&lt;code&gt;Tensor&lt;/code&gt;和 编码其历史 的函数的&lt;code&gt;Function&lt;/code&gt;节点。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;损失函数&#34;&gt;损失函数&lt;/h2&gt;

&lt;p&gt;一个损失函数接受一对 (output, target) 作为输入，计算一个值来估计网络的输出和目标值相差多少。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://pytorch.org/docs/nn&#34; target=&#34;_blank&#34;&gt;nn包&lt;/a&gt;中有很多不同的&lt;a href=&#34;https://pytorch.org/docs/nn.html#loss-functions&#34; target=&#34;_blank&#34;&gt;损失函数&lt;/a&gt;。 &lt;code&gt;nn.MSELoss&lt;/code&gt;是一个比较简单的损失函数，它计算输出和目标间的&lt;strong&gt;均方误差&lt;/strong&gt;， 例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input = torch.randn(1, 1, 32, 32)
output = net(input)
target = torch.randn(10)  # 随机值作为样例
target = target.view(1, -1)  # 使target和output的shape相同

criterion = nn.MSELoss()
loss = criterion(output, target)
print(loss)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor(1.1509, grad_fn=&amp;lt;MseLossBackward&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;反向传播&#34;&gt;反向传播&lt;/h2&gt;

&lt;p&gt;调用&lt;code&gt;loss.backward()&lt;/code&gt;获得反向传播的误差。&lt;/p&gt;

&lt;p&gt;但是在调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。&lt;/p&gt;

&lt;p&gt;现在，我们将调用&lt;code&gt;loss.backward()&lt;/code&gt;，并查看conv1层的偏差（bias）项在反向传播前后的梯度。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;net.zero_grad()  # 清除梯度

print(&#39;conv1.bias.grad before backward&#39;)
print(net.conv1.bias.grad)

loss.backward()

print(&#39;conv1.bias.grad after backward&#39;)
print(net.conv1.bias.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conv1.bias.grad before backward
None
conv1.bias.grad after backward
tensor([ 0.0027, -0.0142,  0.0197,  0.0021, -0.0018,  0.0001])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;更新权重&#34;&gt;更新权重&lt;/h2&gt;

&lt;p&gt;在实践中最简单的权重更新规则是随机梯度下降（SGD）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; weight = weight - learning_rate * gradient
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以使用简单的Python代码实现这个规则：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包&lt;code&gt;torch.optim&lt;/code&gt;实现了所有的这些规则。 使用它们非常简单：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01)
criterion = nn.MSELoss()

# in your training loop
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()   # Does the update
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning Note-2</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-2/</link>
      <pubDate>Mon, 22 Jul 2019 19:20:28 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-2/</guid>
      <description>

&lt;h1 id=&#34;autograd-自动求导机制&#34;&gt;Autograd: 自动求导机制&lt;/h1&gt;

&lt;p&gt;PyTorch 中所有神经网络的核心是 &lt;code&gt;autograd&lt;/code&gt; 包。 我们先简单介绍一下这个包，然后训练第一个简单的神经网络。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;autograd&lt;/code&gt;包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。&lt;/p&gt;

&lt;h2 id=&#34;张量-tensor&#34;&gt;张量 Tensor&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;是这个包的核心类。如果设置 &lt;code&gt;.requires_grad&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt;，那么将会追踪所有对于该张量的操作。 当完成计算后通过调用 &lt;code&gt;.backward()&lt;/code&gt;，自动计算所有的梯度， 这个张量的所有梯度将会自动积累到 &lt;code&gt;.grad&lt;/code&gt; 属性。&lt;/p&gt;

&lt;p&gt;要阻止张量跟踪历史记录，可以调用&lt;code&gt;.detach()&lt;/code&gt;方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。&lt;/p&gt;

&lt;p&gt;为了防止跟踪历史记录（和使用内存），可以将代码块包装在&lt;code&gt;with torch.no_grad()：&lt;/code&gt;中。 在评估模型时特别有用，因为模型可能具有&lt;code&gt;requires_grad = True&lt;/code&gt;的可训练参数，但是我们不需要梯度计算。&lt;/p&gt;

&lt;p&gt;在自动梯度计算中还有另外一个重要的类&lt;code&gt;Function&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Tensor&lt;/code&gt; 和 &lt;code&gt;Function&lt;/code&gt;互相连接并生成一个非循环图，它表示和存储了完整的计算历史。 每个张量都有一个&lt;code&gt;.grad_fn&lt;/code&gt;属性，这个属性引用了一个创建了&lt;code&gt;Tensor&lt;/code&gt;的&lt;code&gt;Function&lt;/code&gt;（除非这个张量是用户手动创建的，即，这个张量的 &lt;code&gt;grad_fn&lt;/code&gt; 是 &lt;code&gt;None&lt;/code&gt;）。&lt;/p&gt;

&lt;p&gt;如果需要计算导数，你可以在&lt;code&gt;Tensor&lt;/code&gt;上调用&lt;code&gt;.backward()&lt;/code&gt;。 如果&lt;code&gt;Tensor&lt;/code&gt;是一个标量（即它包含一个元素数据）则不需要为&lt;code&gt;backward()&lt;/code&gt;指定任何参数， 但是如果它有更多的元素，你需要指定一个&lt;code&gt;gradient&lt;/code&gt; 参数来匹配张量的形状。&lt;/p&gt;

&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    在其他的文章中你可能会看到说将Tensor包裹到Variable中提供自动梯度计算，Variable 这个在0.41版中已经被标注为过期了，现在可以直接使用Tensor，官方文档在&lt;a href=&#34;https://pytorch.org/docs/stable/autograd.html#variable-deprecated&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;创建一个张量并设置 &lt;code&gt;requires_grad=True&lt;/code&gt; 用来追踪他的计算历史&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.ones(2, 2, requires_grad=True)
print(x)

y = x + 2  # 对x进行操作
print(y)
print(&amp;quot;y.grad_fn: &amp;quot;, y.grad_fn)  # 结果y已经被计算出来了，所以，grad_fn已经被自动生成了

z = y * y * 3  # 对y进行操作
out = z.mean()
print(z)
print(out)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
tensor([[3., 3.],
        [3., 3.]], grad_fn=&amp;lt;AddBackward0&amp;gt;)
y.grad_fn:  &amp;lt;AddBackward0 object at 0x00000202022DAE48&amp;gt;
tensor([[27., 27.],
        [27., 27.]], grad_fn=&amp;lt;MulBackward0&amp;gt;)
tensor(27., grad_fn=&amp;lt;MeanBackward0&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;.requires_grad_( ... )&lt;/code&gt; 可以改变现有张量的 &lt;code&gt;requires_grad&lt;/code&gt;属性。 如果没有指定的话，默认输入的flag是 &lt;code&gt;False&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = torch.ones(2, 2)
a = (a * 3) / (a - 1)
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;False
True
&amp;lt;SumBackward0 object at 0x0000028141CBADA0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;梯度&#34;&gt;梯度&lt;/h2&gt;

&lt;p&gt;反向传播 因为 &lt;code&gt;out&lt;/code&gt;是一个纯量（scalar），&lt;code&gt;out.backward()&lt;/code&gt; 等于&lt;code&gt;out.backward(torch.tensor(1))&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;out.backward()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;print gradients $\frac{d(out)}{dx}$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.ones(2, 2, requires_grad=True)
y = x + 2  
z = y * y * 3  
out = z.mean()

out.backward()
print(x.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以用自动求导做更多操作&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn(3, requires_grad=True)

y = x * 2
while y.data.norm() &amp;lt; 1000:
    y = y * 2

print(y)
gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
y.backward(gradients)

print(x.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([-790.8533,  793.1236,  307.1018], grad_fn=&amp;lt;MulBackward0&amp;gt;)
tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果&lt;code&gt;.requires_grad=True&lt;/code&gt;但是你又不希望进行autograd的计算， 那么可以将变量包裹在 &lt;code&gt;with torch.no_grad()&lt;/code&gt;中:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print(x.requires_grad)
    print((x ** 2).requires_grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;True
True
True
False
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;更多阅读：&lt;/strong&gt; autograd 和 Function 的&lt;a href=&#34;https://pytorch.org/docs/autograd&#34; target=&#34;_blank&#34;&gt;官方文档&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning Note-1</title>
      <link>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-1/</link>
      <pubDate>Mon, 22 Jul 2019 18:11:52 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/post/pytorch-learning/note-1/</guid>
      <description>

&lt;h2 id=&#34;pytorch-是什么&#34;&gt;PyTorch 是什么?&lt;/h2&gt;

&lt;p&gt;基于Python的科学计算包，服务于以下两种场景:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;作为NumPy的替代品，可以使用GPU的强大计算能力&lt;/li&gt;
&lt;li&gt;提供最大的灵活性和高速的深度学习研究平台&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tensor-张量&#34;&gt;Tensor 张量&lt;/h2&gt;

&lt;p&gt;Tensors与Numpy中的 ndarrays类似，但是在PyTorch中 Tensors 可以使用GPU进行计算.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from __future__ import print_function
import torch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建一个  5x5 的矩阵，但未初始化:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.empty(5, 3)
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[9.5511e-39, 1.0102e-38, 4.6837e-39],
        [4.9592e-39, 5.0510e-39, 9.9184e-39],
        [9.0000e-39, 1.0561e-38, 1.0653e-38],
        [4.1327e-39, 8.9082e-39, 9.8265e-39],
        [9.4592e-39, 1.0561e-38, 1.0653e-38]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建一个随机初始化的矩阵:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.rand(5, 3)
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[0.6004, 0.9095, 0.5525],
        [0.2870, 0.2680, 0.1937],
        [0.9153, 0.0150, 0.5165],
        [0.7875, 0.7397, 0.9305],
        [0.8575, 0.1453, 0.2655]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建一个0填充的矩阵，数据类型为&lt;code&gt;long&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.zeros(5, 3, dtype=torch.long)
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建tensor并使用现有数据初始化:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.tensor([5.5, 3])
print(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([5.5000, 3.0000])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;根据现有的张量创建张量&lt;/strong&gt;。 这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.tensor([5.5, 3])
print(x)

x = x.new_ones(5, 3, dtype=torch.double)      # new_* 方法来创建对象
print(x)

x = torch.randn_like(x, dtype=torch.float)    # 覆盖 dtype!
print(x)                                      #  对象的size 是相同的，只是值和类型发生了变化
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([5.5000, 3.0000])
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[-0.5648,  1.4639, -0.1247],
        [ 0.4187,  0.0255, -0.0938],
        [-1.2237,  0.3889,  0.9847],
        [-0.2423, -3.3706, -0.3511],
        [-1.1498, -1.1044,  0.4582]])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;获取-size&#34;&gt;获取 size&lt;/h2&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    使用&lt;code&gt;size&lt;/code&gt;方法与Numpy的&lt;code&gt;shape&lt;/code&gt;属性返回的相同，张量也支持&lt;code&gt;shape&lt;/code&gt;属性
  &lt;/div&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.ones(5, 3)
print(x)

print(&amp;quot;x.size(): &amp;quot;, x.size())
print(&amp;quot;x.shape:  &amp;quot;, x.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
x.size():  torch.Size([5, 3])
x.shape:   torch.Size([5, 3])
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;code&gt;torch.Size()&lt;/code&gt;返回值是&lt;code&gt;tuple&lt;/code&gt;类型，所以它支持&lt;code&gt;tuple&lt;/code&gt;类型的所有操作
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;operation-操作&#34;&gt;Operation 操作&lt;/h2&gt;

&lt;h3 id=&#34;加法&#34;&gt;加法&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.rand(5, 3)
y = torch.rand(5, 3)

sum = x + y                 # 加法1，操作符
sum = torch.add(x, y)       # 加法2，函数
torch.add(x, y, out=sum)    # 加法3，提供输出张量sum作为参数
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;替换&#34;&gt;替换&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# add x to y
y.add_(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    任何 以&lt;code&gt;_&lt;/code&gt; 结尾的操作都会用结果替换原变量. 例如: &lt;code&gt;x.copy_(y)&lt;/code&gt;, &lt;code&gt;x.t_()&lt;/code&gt;, 都会改变 &lt;code&gt;x&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;截取&#34;&gt;截取&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(x[:, 1])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;view-reshape&#34;&gt;view / reshape&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;torch.view&lt;/code&gt; 可以改变张量的维度和大小，与numpy的reshape类似&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  #  size -1 从其他维度推断
print(x.size(), y.size(), z.size())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;只有一个元素的张量取值&#34;&gt;只有一个元素的张量取值&lt;/h3&gt;

&lt;p&gt;如果你有只有一个元素的张量，使用&lt;code&gt;.item()&lt;/code&gt;来得到Python数据类型的数值&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn(1)
print(x)
print(x.item())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([-0.2036])
-0.203627809882164
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    更多操作&lt;a href=&#34;https://pytorch.org/docs/stable/torch.html&#34; target=&#34;_blank&#34;&gt;点击此处&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;numpy-转换&#34;&gt;Numpy 转换&lt;/h2&gt;

&lt;p&gt;Torch Tensor与NumPy数组&lt;strong&gt;共享底层内存地址&lt;/strong&gt;，修改一个会导致另一个的变化。&lt;/p&gt;

&lt;h3 id=&#34;torch-tensor-转换成-numpy数组&#34;&gt;Torch Tensor 转换成 NumPy数组&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = torch.ones(5)
print(a)

b = a.numpy()
print(b)

a.add_(1)
print(a)
print(b)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([1., 1., 1., 1., 1.])
[1. 1. 1. 1. 1.]
tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;numpy数组-转换成-torch-tensor&#34;&gt;NumPy数组 转换成 Torch Tensor&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = np.ones(5)
b = torch.from_numpy(a)
print(a)
print(b)

np.add(a, 1, out=a)
print(a)
print(b)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[1. 1. 1. 1. 1.]
tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    所有的 &lt;code&gt;Tensor&lt;/code&gt; 类型默认都是基于CPU， &lt;code&gt;CharTensor&lt;/code&gt; 类型不支持到 NumPy 的转换.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;cuda-张量&#34;&gt;CUDA 张量&lt;/h2&gt;

&lt;p&gt;使用&lt;code&gt;.to&lt;/code&gt; 方法 可以将Tensor移动到任何设备中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.rand(1)

# is_available 函数判断是否有cuda可以使用
# torch.device 将张量移动到指定的设备中
if torch.cuda.is_available():
    device = torch.device(&amp;quot;cuda&amp;quot;)          # a CUDA 设备对象
    y = torch.ones_like(x, device=device)  # 直接从GPU创建张量
    x = x.to(device)                       # 或者直接使用 .to(&amp;quot;cuda&amp;quot;) 将张量移动到cuda中
    z = x + y
    print(z)
    print(z.to(&amp;quot;cpu&amp;quot;, torch.double))       # .to 也会对变量的类型做更改
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([1.2840], device=&#39;cuda:0&#39;)
tensor([1.2840], dtype=torch.float64)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Learning</title>
      <link>https://YuxinZhaozyx.github.io/project/pytorch-learning/</link>
      <pubDate>Mon, 22 Jul 2019 17:37:53 +0800</pubDate>
      
      <guid>https://YuxinZhaozyx.github.io/project/pytorch-learning/</guid>
      <description>

&lt;p&gt;This project is to record my learning road of pytorch framework.&lt;/p&gt;

&lt;h2 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34; target=&#34;_blank&#34;&gt;官方教程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/zergtant/pytorch-handbook&#34; target=&#34;_blank&#34;&gt;PyTorch中文手册&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
